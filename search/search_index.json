{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"popup-ai","text":"<p>Real-time audio transcription and annotation overlay system for live streaming.</p> <p>popup-ai captures audio from your stream via SRT, transcribes it using Whisper, extracts key terms with an LLM, and displays educational annotations as overlays in OBS.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Real-time transcription using mlx-whisper on Apple Silicon</li> <li>Automatic term extraction via pydantic-ai with configurable LLM providers</li> <li>OBS overlay integration for displaying annotations during streams</li> <li>Admin UI for monitoring and control</li> <li>Modular architecture with Ray actors for fault isolation</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install with all extras\nuv pip install 'popup-ai[all]'\n\n# Start the application\nuv run popup-ai\n</code></pre> <p>This opens the admin UI at <code>http://127.0.0.1:8080</code>. Configure OBS to stream audio via SRT, then click \"Start Pipeline\" to begin.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>This documentation follows the Diataxis framework:</p> <ul> <li> <p>:material-school: Tutorials</p> <p>Learning-oriented lessons to get you started</p> <p>:octicons-arrow-right-24: Start learning</p> </li> <li> <p>:material-directions: How-to Guides</p> <p>Task-oriented guides for specific goals</p> <p>:octicons-arrow-right-24: Find a guide</p> </li> <li> <p>:material-book-open-variant: Reference</p> <p>Technical descriptions of CLI, config, and APIs</p> <p>:octicons-arrow-right-24: Browse reference</p> </li> <li> <p>:material-lightbulb: Explanation</p> <p>Conceptual discussions of architecture and design</p> <p>:octicons-arrow-right-24: Understand more</p> </li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>macOS with Apple Silicon (M1/M2/M3)</li> <li>Python 3.13+</li> <li>OBS Studio (for overlay display)</li> <li>ffmpeg (for SRT audio capture)</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Main Process                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                   NiceGUI Admin UI                         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                              \u2502                                   \u2502\n\u2502                              \u25bc                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              PipelineSupervisor (Ray Actor)                \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u25bc                     \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 AudioIngestActor\u2502\u2192 \u2502TranscriberActor \u2502\u2192 \u2502 AnnotatorActor  \u2502\n\u2502   (ffmpeg SRT)  \u2502  \u2502  (mlx-whisper)  \u2502  \u2502  (pydantic-ai)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                   \u2502\n                                                   \u25bc\n                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                          \u2502  OverlayActor   \u2502\n                                          \u2502 (obsws-python)  \u2502\n                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>See Architecture for details.</p>"},{"location":"explanation/","title":"Explanation","text":"<p>Explanation documentation provides conceptual understanding of how popup-ai works and why it's designed the way it is.</p>"},{"location":"explanation/#available-explanations","title":"Available Explanations","text":""},{"location":"explanation/#architecture-overview","title":"Architecture Overview","text":"<p>High-level view of popup-ai's design.</p> <p>Topics covered:</p> <ul> <li>System components</li> <li>Data flow through the pipeline</li> <li>How the UI integrates with actors</li> <li>Design decisions and trade-offs</li> </ul>"},{"location":"explanation/#ray-actors","title":"Ray Actors","text":"<p>Deep dive into the Ray actor architecture.</p> <p>Topics covered:</p> <ul> <li>Why Ray for this project</li> <li>Actor isolation and fault tolerance</li> <li>Queue-based communication</li> <li>Supervision and restart policies</li> </ul>"},{"location":"explanation/#design-philosophy","title":"Design Philosophy","text":"<p>popup-ai is built on several key principles:</p>"},{"location":"explanation/#modular-monolith","title":"Modular Monolith","text":"<p>A single process with isolated components. Not microservices, but not a tangled monolith either. Each actor is independent and testable.</p>"},{"location":"explanation/#fault-isolation","title":"Fault Isolation","text":"<p>If the LLM annotator fails, transcription continues. If transcription crashes, it restarts without affecting the overlay. Ray actors provide this isolation naturally.</p>"},{"location":"explanation/#ui-independence","title":"UI Independence","text":"<p>The admin UI is always available, even when the pipeline is stopped. You can start, stop, and monitor actors without restarting the application.</p>"},{"location":"explanation/#configuration-over-code","title":"Configuration Over Code","text":"<p>Runtime behavior is controlled via configuration, not code changes. Want to disable the overlay? Set an environment variable. Want a different model? Change a setting.</p>"},{"location":"explanation/architecture/","title":"Architecture Overview","text":"<p>This document explains popup-ai's architecture and the reasoning behind key design decisions.</p>"},{"location":"explanation/architecture/#system-overview","title":"System Overview","text":"<p>popup-ai is a modular monolith - a single process with isolated components that communicate via message queues.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Main Process                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                   NiceGUI Admin UI                         \u2502  \u2502\n\u2502  \u2502  \u2022 Actor status dashboard    \u2022 Settings configuration      \u2502  \u2502\n\u2502  \u2502  \u2022 Pipeline controls         \u2022 Live transcript view        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                              \u2502                                   \u2502\n\u2502                              \u2502 actor handles                     \u2502\n\u2502                              \u25bc                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              PipelineSupervisor (Ray Actor)                \u2502  \u2502\n\u2502  \u2502  \u2022 Spawns/manages child actors    \u2022 Health monitoring      \u2502  \u2502\n\u2502  \u2502  \u2022 Restart policies               \u2022 Graceful shutdown      \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                     \u2502                     \u2502\n         \u25bc                     \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 AudioIngestActor\u2502  \u2502TranscriberActor \u2502  \u2502 AnnotatorActor  \u2502\n\u2502                 \u2502  \u2502                 \u2502  \u2502                 \u2502\n\u2502 ffmpeg SRT\u2192PCM  \u2502\u2500\u25b6\u2502 mlx-whisper     \u2502\u2500\u25b6\u2502 pydantic-ai     \u2502\n\u2502 chunks to queue \u2502  \u2502 PCM\u2192transcript  \u2502  \u2502 text\u2192annotation \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                   \u2502\n                                                   \u25bc\n                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                          \u2502 OverlayActor    \u2502\n                                          \u2502                 \u2502\n                                          \u2502 obsws-python    \u2502\n                                          \u2502 annotation\u2192OBS  \u2502\n                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"explanation/architecture/#components","title":"Components","text":""},{"location":"explanation/architecture/#main-process","title":"Main Process","text":"<p>The main Python process runs:</p> <ul> <li>NiceGUI Admin UI: Web interface for monitoring and control</li> <li>Ray runtime: Manages actor lifecycle and communication</li> </ul>"},{"location":"explanation/architecture/#pipelinesupervisor","title":"PipelineSupervisor","text":"<p>A Ray actor that orchestrates the pipeline:</p> <ul> <li>Creates Ray Queues for inter-actor communication</li> <li>Spawns child actors based on configuration</li> <li>Monitors actor health every 5 seconds</li> <li>Restarts crashed actors automatically</li> <li>Coordinates graceful shutdown</li> </ul>"},{"location":"explanation/architecture/#pipeline-actors","title":"Pipeline Actors","text":"<p>Four specialized actors, each handling one pipeline stage:</p> Actor Responsibility Technology AudioIngestActor Capture SRT audio ffmpeg subprocess TranscriberActor Speech-to-text mlx-whisper AnnotatorActor Term extraction pydantic-ai + SQLite OverlayActor OBS display obsws-python"},{"location":"explanation/architecture/#data-flow","title":"Data Flow","text":"<p>Data flows through the pipeline via Ray Queues:</p> <pre><code>SRT Stream\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 AudioIngestActor\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 AudioChunk\n         \u25bc\n    [audio_queue]\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 TranscriberActor\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 Transcript\n         \u25bc\n  [transcript_queue]\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 AnnotatorActor  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 Annotation\n         \u25bc\n  [annotation_queue]\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  OverlayActor   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n    OBS WebSocket\n</code></pre>"},{"location":"explanation/architecture/#message-types","title":"Message Types","text":"Type Fields Flow AudioChunk data, timestamp_ms, sample_rate, channels AudioIngest \u2192 Transcriber Transcript text, segments, is_partial, timestamp_ms Transcriber \u2192 Annotator Annotation term, explanation, slot, display_duration_ms Annotator \u2192 Overlay UIEvent source, event_type, data, timestamp_ms All actors \u2192 UI"},{"location":"explanation/architecture/#queue-properties","title":"Queue Properties","text":"<ul> <li>Bounded capacity: 100 items (audio, transcript, annotation), 1000 (ui)</li> <li>Backpressure: Producers drop items if queue is full</li> <li>Async-compatible: Non-blocking put/get operations</li> </ul>"},{"location":"explanation/architecture/#ui-integration","title":"UI Integration","text":"<p>The admin UI runs in the main process and communicates with actors via:</p> <ol> <li>Actor handles: Direct method calls (start, stop, get_status)</li> <li>UI event queue: Actors push events, UI polls for updates</li> </ol> <pre><code># UI calling actor method\nawait supervisor.start.remote()\n\n# UI receiving events\nevent = ui_queue.get_nowait()\nif event.source == \"transcriber\":\n    transcript_log.push(event.data[\"text\"])\n</code></pre> <p>This design means:</p> <ul> <li>UI is always responsive (not blocked by actor work)</li> <li>UI can start/stop without affecting running actors</li> <li>Actor crashes don't crash the UI</li> </ul>"},{"location":"explanation/architecture/#design-decisions","title":"Design Decisions","text":""},{"location":"explanation/architecture/#why-ray","title":"Why Ray?","text":"<p>We evaluated several options for actor-based concurrency:</p> Option Pros Cons asyncio Simple, no deps No isolation, complex error handling multiprocessing Isolation Complex IPC, no supervision Celery Battle-tested Heavy, needs Redis/RabbitMQ Ray Isolation, supervision, queues Learning curve, memory overhead <p>Ray won because:</p> <ul> <li>Actor isolation: Each actor runs in its own process</li> <li>Built-in queues: <code>ray.util.queue.Queue</code> for backpressure-aware messaging</li> <li>Supervision: Detect crashes, get actor state</li> <li>Apple Silicon: Good performance on M1/M2/M3</li> </ul>"},{"location":"explanation/architecture/#why-modular-monolith","title":"Why Modular Monolith?","text":"<p>We considered microservices but chose a monolith because:</p> <ul> <li>Simpler deployment: One process to manage</li> <li>Lower latency: No network hops between components</li> <li>Easier debugging: Single process, shared logs</li> <li>Good enough isolation: Ray actors provide fault boundaries</li> </ul> <p>The architecture is modular enough that individual actors could be extracted to separate services if needed.</p>"},{"location":"explanation/architecture/#why-nicegui","title":"Why NiceGUI?","text":"<p>For the admin UI, we chose NiceGUI over alternatives:</p> Option Pros Cons FastAPI + React Full control Two codebases, more complexity Streamlit Easy Stateless, reruns on every change Gradio ML-focused Limited customization NiceGUI Python-only, reactive Less mature ecosystem <p>NiceGUI lets us build the entire UI in Python with reactive updates.</p>"},{"location":"explanation/architecture/#why-sqlite-for-cache","title":"Why SQLite for Cache?","text":"<p>The annotator caches LLM results in SQLite:</p> <ul> <li>Zero config: No database server needed</li> <li>Persistent: Survives restarts</li> <li>Fast: Local file, no network</li> <li>Simple: Single table, key-value pattern</li> </ul> <p>For v1, this is sufficient. Future versions may add Redis for distributed caching.</p>"},{"location":"explanation/architecture/#failure-modes","title":"Failure Modes","text":""},{"location":"explanation/architecture/#actor-crash","title":"Actor Crash","text":"<ol> <li>Supervisor detects crash via health check</li> <li>Supervisor respawns the actor</li> <li>Actor restarts with fresh state</li> <li>Data in queues is preserved</li> </ol>"},{"location":"explanation/architecture/#obs-disconnection","title":"OBS Disconnection","text":"<ol> <li>OverlayActor detects connection failure</li> <li>Continues processing (annotations queued)</li> <li>Reconnects when OBS is available</li> <li>Resumes sending annotations</li> </ol>"},{"location":"explanation/architecture/#llm-api-failure","title":"LLM API Failure","text":"<ol> <li>AnnotatorActor catches exception</li> <li>Logs error, continues processing</li> <li>No annotation generated for that transcript</li> <li>Pipeline continues with next transcript</li> </ol>"},{"location":"explanation/architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"explanation/architecture/#memory","title":"Memory","text":"<ul> <li>Each actor runs in a separate process</li> <li>Whisper models load into GPU memory</li> <li>SQLite cache grows with unique transcripts</li> <li>Queues are bounded to prevent unbounded growth</li> </ul>"},{"location":"explanation/architecture/#latency","title":"Latency","text":"<p>Approximate latency breakdown:</p> Stage Typical Latency SRT ingest 200ms (configurable) Audio buffering 5s (chunk_length) Whisper transcription 1-5s (depends on model) LLM annotation 1-3s (depends on provider) OBS update &lt;100ms <p>Total end-to-end: 7-15 seconds from speech to overlay.</p>"},{"location":"explanation/architecture/#throughput","title":"Throughput","text":"<ul> <li>Audio: Real-time (1x)</li> <li>Transcription: ~0.5-2x real-time (depends on model)</li> <li>Annotation: Limited by LLM API rate limits</li> </ul>"},{"location":"explanation/architecture/#see-also","title":"See Also","text":"<ul> <li>Ray Actors - Deep dive into Ray</li> <li>Actors Reference - Technical details</li> <li>Configuration - All settings</li> </ul>"},{"location":"explanation/ray-actors/","title":"Ray Actors","text":"<p>This document explains why popup-ai uses Ray actors and how they work.</p>"},{"location":"explanation/ray-actors/#what-is-ray","title":"What is Ray?","text":"<p>Ray is a framework for building distributed applications. popup-ai uses Ray's actor model for:</p> <ul> <li>Process isolation: Each actor runs in its own process</li> <li>Fault tolerance: Crashed actors can be detected and restarted</li> <li>Message passing: Queues for inter-actor communication</li> <li>Async/await: Native async support in actors</li> </ul>"},{"location":"explanation/ray-actors/#why-actors","title":"Why Actors?","text":"<p>The popup-ai pipeline has natural stage boundaries:</p> <pre><code>Audio Capture \u2192 Transcription \u2192 Annotation \u2192 Overlay\n</code></pre> <p>Each stage:</p> <ul> <li>Has different failure modes</li> <li>Runs at different speeds</li> <li>Can be developed/tested independently</li> <li>Should not block other stages</li> </ul> <p>Actors provide this isolation naturally. If the LLM API fails, transcription continues. If Whisper crashes, it restarts without affecting the overlay.</p>"},{"location":"explanation/ray-actors/#actor-basics","title":"Actor Basics","text":""},{"location":"explanation/ray-actors/#defining-an-actor","title":"Defining an Actor","text":"<p>Ray actors are Python classes decorated with <code>@ray.remote</code>:</p> <pre><code>import ray\n\n@ray.remote\nclass MyActor:\n    def __init__(self, config):\n        self.config = config\n        self.state = {}\n\n    def get_state(self):\n        return self.state\n\n    async def process(self, data):\n        # Do work\n        result = await expensive_operation(data)\n        return result\n</code></pre>"},{"location":"explanation/ray-actors/#creating-actors","title":"Creating Actors","text":"<p>Actors are created with <code>.remote()</code>:</p> <pre><code># Create an actor instance\nactor = MyActor.remote(config)\n\n# Call methods with .remote()\nresult = await actor.process.remote(data)\n\n# Get return value with ray.get()\nstate = ray.get(actor.get_state.remote())\n</code></pre>"},{"location":"explanation/ray-actors/#actor-lifecycle","title":"Actor Lifecycle","text":"<pre><code>Created \u2192 Running \u2192 Stopped\n              \u2193\n           Crashed \u2192 Restarted\n</code></pre> <p>popup-ai's supervisor manages this lifecycle for all pipeline actors.</p>"},{"location":"explanation/ray-actors/#queue-communication","title":"Queue Communication","text":""},{"location":"explanation/ray-actors/#why-queues","title":"Why Queues?","text":"<p>Actors could call each other directly:</p> <pre><code># Direct call (not recommended)\ntranscript = await transcriber.process.remote(audio)\nannotation = await annotator.process.remote(transcript)\n</code></pre> <p>But this creates tight coupling. Instead, we use queues:</p> <pre><code># Queue-based (recommended)\naudio_queue.put(audio_chunk)\n# Transcriber pulls from audio_queue, pushes to transcript_queue\n# Annotator pulls from transcript_queue, pushes to annotation_queue\n</code></pre> <p>Benefits:</p> <ul> <li>Decoupling: Actors don't know about each other</li> <li>Backpressure: Bounded queues prevent memory explosion</li> <li>Buffering: Fast producers don't overwhelm slow consumers</li> <li>Observability: Queue depths indicate pipeline health</li> </ul>"},{"location":"explanation/ray-actors/#ray-queue-api","title":"Ray Queue API","text":"<pre><code>from ray.util.queue import Queue\n\n# Create bounded queue\nqueue = Queue(maxsize=100)\n\n# Non-blocking put (raises if full)\nqueue.put_nowait(item)\n\n# Blocking get with timeout\nitem = queue.get(block=True, timeout=0.5)\n\n# Non-blocking get (raises if empty)\nitem = queue.get_nowait()\n</code></pre>"},{"location":"explanation/ray-actors/#queue-topology","title":"Queue Topology","text":"<p>popup-ai creates four queues:</p> <pre><code>audio_queue (100)     : AudioIngest \u2192 Transcriber\ntranscript_queue (100): Transcriber \u2192 Annotator\nannotation_queue (100): Annotator \u2192 Overlay\nui_queue (1000)       : All actors \u2192 Admin UI\n</code></pre>"},{"location":"explanation/ray-actors/#supervision","title":"Supervision","text":""},{"location":"explanation/ray-actors/#health-monitoring","title":"Health Monitoring","text":"<p>The supervisor checks actor health every 5 seconds:</p> <pre><code>async def _health_monitor(self):\n    while self._running:\n        await asyncio.sleep(5)\n\n        for name, actor in self._actors.items():\n            try:\n                healthy = await actor.health_check.remote()\n                if not healthy:\n                    await self.restart_actor(name)\n            except ray.exceptions.RayActorError:\n                # Actor crashed\n                await self._spawn_actor(name)\n</code></pre>"},{"location":"explanation/ray-actors/#health-check-implementation","title":"Health Check Implementation","text":"<p>Each actor implements a simple health check:</p> <pre><code>def health_check(self) -&gt; bool:\n    return self._state == \"running\" and self._running\n</code></pre> <p>Actors can report unhealthy if:</p> <ul> <li>Internal error state</li> <li>External dependency unavailable</li> <li>Processing stalled</li> </ul>"},{"location":"explanation/ray-actors/#restart-policy","title":"Restart Policy","text":"<p>On crash or unhealthy status:</p> <ol> <li>Old actor is killed (<code>ray.kill()</code>)</li> <li>New actor is spawned with same config</li> <li>Actor reconnects to queues</li> <li>Processing resumes</li> </ol> <p>Data in queues is preserved across restarts.</p>"},{"location":"explanation/ray-actors/#error-handling","title":"Error Handling","text":""},{"location":"explanation/ray-actors/#actor-crashes","title":"Actor Crashes","text":"<p>Ray detects when an actor process dies:</p> <pre><code>try:\n    await actor.method.remote()\nexcept ray.exceptions.RayActorError:\n    # Actor crashed\n    pass\n</code></pre> <p>The supervisor catches this and restarts the actor.</p>"},{"location":"explanation/ray-actors/#method-exceptions","title":"Method Exceptions","text":"<p>Exceptions in actor methods are captured and re-raised to callers:</p> <pre><code>@ray.remote\nclass MyActor:\n    async def risky_method(self):\n        raise ValueError(\"Something went wrong\")\n\n# Caller sees the exception\ntry:\n    await actor.risky_method.remote()\nexcept ValueError:\n    # Handle error\n    pass\n</code></pre>"},{"location":"explanation/ray-actors/#graceful-degradation","title":"Graceful Degradation","text":"<p>popup-ai actors are designed to degrade gracefully:</p> <ul> <li>AudioIngest: If ffmpeg dies, reports error but doesn't crash</li> <li>Transcriber: If Whisper fails, logs error and continues</li> <li>Annotator: If LLM fails, skips annotation and continues</li> <li>Overlay: If OBS disconnects, continues processing</li> </ul>"},{"location":"explanation/ray-actors/#performance","title":"Performance","text":""},{"location":"explanation/ray-actors/#process-overhead","title":"Process Overhead","text":"<p>Each actor is a separate Python process:</p> <ul> <li>~50-100MB base memory per actor</li> <li>Context switch overhead for queue operations</li> <li>Worth it for isolation benefits</li> </ul>"},{"location":"explanation/ray-actors/#serialization","title":"Serialization","text":"<p>Data crossing actor boundaries is serialized:</p> <ul> <li>Small messages (AudioChunk): Fast, minimal overhead</li> <li>Large objects: Consider memory-mapped files or object store</li> </ul> <p>popup-ai's messages are small Pydantic models, so serialization is fast.</p>"},{"location":"explanation/ray-actors/#local-vs-distributed","title":"Local vs. Distributed","text":"<p>popup-ai runs Ray in local mode:</p> <pre><code>ray.init(ignore_reinit_error=True, logging_level=logging.WARNING)\n</code></pre> <p>For single-machine use, this is optimal. Ray can scale to clusters if needed.</p>"},{"location":"explanation/ray-actors/#comparison-to-alternatives","title":"Comparison to Alternatives","text":""},{"location":"explanation/ray-actors/#vs-asyncio","title":"vs. asyncio","text":"<pre><code># asyncio approach\nasync def pipeline():\n    audio = await capture_audio()\n    transcript = await transcribe(audio)\n    annotation = await annotate(transcript)\n    await display(annotation)\n</code></pre> <p>Problems:</p> <ul> <li>One crash takes down everything</li> <li>No process isolation</li> <li>Complex error handling</li> </ul>"},{"location":"explanation/ray-actors/#vs-multiprocessing","title":"vs. multiprocessing","text":"<pre><code># multiprocessing approach\naudio_queue = multiprocessing.Queue()\ntranscript_queue = multiprocessing.Queue()\n\nProcess(target=transcribe, args=(audio_queue, transcript_queue)).start()\n</code></pre> <p>Problems:</p> <ul> <li>Manual process management</li> <li>No supervision</li> <li>Complex IPC patterns</li> </ul>"},{"location":"explanation/ray-actors/#vs-celery","title":"vs. Celery","text":"<pre><code># Celery approach\n@celery.task\ndef transcribe(audio):\n    return whisper.transcribe(audio)\n</code></pre> <p>Problems:</p> <ul> <li>Requires Redis/RabbitMQ</li> <li>Heavier deployment</li> <li>Task-based, not actor-based</li> </ul>"},{"location":"explanation/ray-actors/#ray-wins","title":"Ray Wins","text":"<p>Ray provides:</p> <ul> <li>Actor model (stateful processes)</li> <li>Built-in supervision (detect crashes)</li> <li>Simple queues (no external deps)</li> <li>Async/await native</li> <li>Good local performance</li> </ul>"},{"location":"explanation/ray-actors/#further-reading","title":"Further Reading","text":"<ul> <li>Ray Documentation</li> <li>Ray Actors Guide</li> <li>Ray Queues</li> <li>Architecture Overview - How actors fit into popup-ai</li> </ul>"},{"location":"how-to/","title":"How-to Guides","text":"<p>How-to guides are task-oriented recipes that guide you through the steps to solve specific problems. They assume you have some basic understanding of popup-ai.</p>"},{"location":"how-to/#available-guides","title":"Available Guides","text":""},{"location":"how-to/#install-ffmpeg-with-srt-support","title":"Install ffmpeg with SRT Support","text":"<p>Install ffmpeg with the required SRT protocol support for audio streaming.</p> <p>You'll learn how to:</p> <ul> <li>Install ffmpeg from homebrew-ffmpeg tap</li> <li>Verify SRT support is enabled</li> <li>Choose optional build flags</li> <li>Troubleshoot build issues</li> </ul>"},{"location":"how-to/#configure-obs-for-srt-streaming","title":"Configure OBS for SRT Streaming","text":"<p>Set up OBS Studio to stream audio to popup-ai via SRT protocol.</p> <p>You'll learn how to:</p> <ul> <li>Configure OBS streaming settings</li> <li>Set optimal SRT parameters</li> <li>Test the connection</li> <li>Troubleshoot common issues</li> </ul>"},{"location":"how-to/#tune-transcription-settings","title":"Tune Transcription Settings","text":"<p>Optimize transcription quality for your content and hardware.</p> <p>You'll learn how to:</p> <ul> <li>Choose the right Whisper model</li> <li>Adjust chunk and overlap settings</li> <li>Balance latency vs. accuracy</li> <li>Configure language detection</li> </ul>"},{"location":"how-to/#run-pipeline-subsets","title":"Run Pipeline Subsets","text":"<p>Run only the pipeline stages you need for specific workflows.</p> <p>You'll learn how to:</p> <ul> <li>Enable/disable specific actors</li> <li>Use environment variables for configuration</li> <li>Common subset patterns</li> </ul>"},{"location":"how-to/#quick-reference","title":"Quick Reference","text":"Task Guide Install ffmpeg with SRT Install ffmpeg Connect OBS to popup-ai Configure OBS Improve transcription accuracy Tune Transcription Run without OBS overlay Pipeline Subsets"},{"location":"how-to/configure-obs/","title":"Configure OBS for SRT Streaming","text":"<p>This guide walks you through configuring OBS Studio to stream audio to popup-ai via SRT.</p>"},{"location":"how-to/configure-obs/#prerequisites","title":"Prerequisites","text":"<ul> <li>OBS Studio 28.0+ (SRT support built-in)</li> <li>popup-ai installed and running</li> <li>Network access between OBS and popup-ai (localhost or LAN)</li> </ul>"},{"location":"how-to/configure-obs/#basic-configuration","title":"Basic Configuration","text":""},{"location":"how-to/configure-obs/#step-1-open-stream-settings","title":"Step 1: Open Stream Settings","text":"<ol> <li>Open OBS Studio</li> <li>Go to Settings \u2192 Stream</li> </ol>"},{"location":"how-to/configure-obs/#step-2-configure-custom-server","title":"Step 2: Configure Custom Server","text":"<ol> <li>Set Service to <code>Custom...</code></li> <li>Set Server to:</li> </ol> <pre><code>srt://localhost:9998?mode=caller&amp;latency=200000\n</code></pre> <ol> <li>Leave Stream Key empty</li> <li>Click Apply</li> </ol>"},{"location":"how-to/configure-obs/#understanding-the-srt-url","title":"Understanding the SRT URL","text":"Parameter Value Meaning <code>srt://</code> Protocol SRT streaming protocol <code>localhost</code> Host Where popup-ai is running <code>9998</code> Port popup-ai's SRT listener port <code>mode=caller</code> Mode OBS initiates the connection <code>latency=200000</code> Latency 200ms buffer (in microseconds)"},{"location":"how-to/configure-obs/#audio-configuration","title":"Audio Configuration","text":""},{"location":"how-to/configure-obs/#step-3-configure-audio-output","title":"Step 3: Configure Audio Output","text":"<p>Go to Settings \u2192 Output \u2192 Streaming:</p> <ol> <li>Set Audio Encoder to <code>AAC</code></li> <li>Set Audio Bitrate to <code>128</code> (or higher for better quality)</li> </ol> <p>Audio Quality</p> <p>Higher bitrates improve transcription quality but use more bandwidth. 128kbps is a good balance.</p>"},{"location":"how-to/configure-obs/#step-4-verify-audio-sources","title":"Step 4: Verify Audio Sources","text":"<p>In the main OBS window, check your Audio Mixer:</p> <ul> <li>Ensure your microphone/audio source is active</li> <li>Levels should be visible when speaking</li> <li>Avoid clipping (staying in the red)</li> </ul>"},{"location":"how-to/configure-obs/#remote-streaming","title":"Remote Streaming","text":"<p>If OBS and popup-ai are on different machines:</p>"},{"location":"how-to/configure-obs/#on-the-popup-ai-machine","title":"On the popup-ai machine","text":"<p>Ensure the SRT port is accessible:</p> <pre><code># Check if port is listening\nlsof -i :9998\n</code></pre>"},{"location":"how-to/configure-obs/#on-the-obs-machine","title":"On the OBS machine","text":"<p>Update the SRT URL with the remote IP:</p> <pre><code>srt://192.168.1.100:9998?mode=caller&amp;latency=200000\n</code></pre> <p>Replace <code>192.168.1.100</code> with the popup-ai machine's IP address.</p> <p>Firewall</p> <p>Make sure port 9998 is open on the popup-ai machine's firewall.</p>"},{"location":"how-to/configure-obs/#latency-tuning","title":"Latency Tuning","text":"<p>The <code>latency</code> parameter affects the trade-off between delay and reliability:</p> Latency Delay Reliability Use Case <code>120000</code> 120ms Lower Local network, low jitter <code>200000</code> 200ms Medium Recommended default <code>500000</code> 500ms Higher Unstable network"},{"location":"how-to/configure-obs/#symptoms-of-wrong-latency","title":"Symptoms of Wrong Latency","text":"<p>Too low: - Audio glitches and dropouts - Disconnections</p> <p>Too high: - Noticeable delay in transcription - Annotations appear late</p>"},{"location":"how-to/configure-obs/#testing-the-connection","title":"Testing the Connection","text":""},{"location":"how-to/configure-obs/#step-1-start-popup-ai","title":"Step 1: Start popup-ai","text":"<pre><code>uv run popup-ai\n</code></pre> <p>Verify the SRT listener is ready by checking the terminal output.</p>"},{"location":"how-to/configure-obs/#step-2-start-obs-streaming","title":"Step 2: Start OBS Streaming","text":"<ol> <li>Click Start Streaming in OBS</li> <li>Watch the OBS status bar for connection confirmation</li> <li>Check popup-ai admin UI for incoming audio</li> </ol>"},{"location":"how-to/configure-obs/#step-3-verify-audio-flow","title":"Step 3: Verify Audio Flow","text":"<p>In popup-ai admin UI:</p> <ul> <li>Audio Ingest actor should show green</li> <li><code>chunks_sent</code> stat should be increasing</li> <li>Live Transcript should show text (once Transcriber processes)</li> </ul>"},{"location":"how-to/configure-obs/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/configure-obs/#connection-refused","title":"\"Connection refused\"","text":"<ol> <li>Ensure popup-ai is running before OBS starts streaming</li> <li>Check the port number matches (default: 9998)</li> <li>Verify no firewall blocking the port</li> </ol>"},{"location":"how-to/configure-obs/#obs-shows-connecting-indefinitely","title":"OBS shows \"Connecting...\" indefinitely","text":"<ol> <li>Check the SRT URL is correct</li> <li>Verify <code>mode=caller</code> is set</li> <li>Try increasing latency</li> </ol>"},{"location":"how-to/configure-obs/#audio-glitches-in-transcription","title":"Audio glitches in transcription","text":"<ol> <li>Increase latency to 300000 or higher</li> <li>Check network stability</li> <li>Reduce OBS output resolution to lower CPU usage</li> </ol>"},{"location":"how-to/configure-obs/#no-audio-reaching-popup-ai","title":"No audio reaching popup-ai","text":"<ol> <li>Verify OBS audio mixer shows levels</li> <li>Check OBS is actually streaming (status bar)</li> <li>Restart both OBS and popup-ai</li> </ol>"},{"location":"how-to/configure-obs/#advanced-multiple-audio-tracks","title":"Advanced: Multiple Audio Tracks","text":"<p>OBS can send multiple audio tracks. By default, popup-ai uses Track 1.</p> <p>To configure which tracks OBS sends:</p> <ol> <li>Go to Settings \u2192 Output \u2192 Streaming</li> <li>Under Audio Track, select the tracks to include</li> </ol> <p>Future Feature</p> <p>Multi-track selection in popup-ai is planned for a future release.</p>"},{"location":"how-to/configure-obs/#whats-next","title":"What's Next?","text":"<ul> <li>Tune Transcription - Optimize transcription quality</li> <li>Configuration Reference - All audio settings</li> </ul>"},{"location":"how-to/install-ffmpeg/","title":"Install ffmpeg with SRT Support","text":"<p>popup-ai uses ffmpeg to receive audio streams via SRT (Secure Reliable Transport) protocol. The standard Homebrew ffmpeg does not include SRT support, so you need to install a custom build.</p>"},{"location":"how-to/install-ffmpeg/#quick-install-macos","title":"Quick Install (macOS)","text":"<pre><code># Add the homebrew-ffmpeg tap\nbrew tap homebrew-ffmpeg/ffmpeg\n\n# Install with SRT support\nbrew install homebrew-ffmpeg/ffmpeg/ffmpeg --with-srt\n</code></pre>"},{"location":"how-to/install-ffmpeg/#if-you-already-have-ffmpeg-installed","title":"If You Already Have ffmpeg Installed","text":"<pre><code># Unlink the existing ffmpeg\nbrew unlink ffmpeg\n\n# Install the custom build with SRT\nbrew install homebrew-ffmpeg/ffmpeg/ffmpeg --with-srt\n</code></pre>"},{"location":"how-to/install-ffmpeg/#verify-installation","title":"Verify Installation","text":"<p>Check that SRT protocol is available:</p> <pre><code>ffmpeg -protocols 2&gt;&amp;1 | grep -E \"^\\s+srt$\"\n</code></pre> <p>You should see <code>srt</code> in the output. If not, the installation didn't include SRT support.</p> <p>Check the version and build configuration:</p> <pre><code>ffmpeg -version\n</code></pre> <p>Look for <code>--enable-libsrt</code> in the configuration line.</p>"},{"location":"how-to/install-ffmpeg/#recommended-build-options","title":"Recommended Build Options","text":"<p>For popup-ai, the minimal install is:</p> <pre><code>brew install homebrew-ffmpeg/ffmpeg/ffmpeg --with-srt\n</code></pre> <p>For better audio quality, add these options:</p> <pre><code>brew install homebrew-ffmpeg/ffmpeg/ffmpeg \\\n    --with-srt \\\n    --with-fdk-aac \\\n    --with-libsoxr\n</code></pre> Option Purpose <code>--with-srt</code> Required. SRT protocol for OBS streaming <code>--with-fdk-aac</code> Higher quality AAC codec (if OBS uses AAC audio) <code>--with-libsoxr</code> High-quality audio resampling"},{"location":"how-to/install-ffmpeg/#all-available-options","title":"All Available Options","text":"<p>To see all available build options:</p> <pre><code>brew options homebrew-ffmpeg/ffmpeg/ffmpeg\n</code></pre> <p>Notable options for audio/video work:</p> Option Description <code>--with-srt</code> SRT streaming protocol <code>--with-fdk-aac</code> Fraunhofer AAC codec <code>--with-libsoxr</code> High-quality resampling <code>--with-openssl</code> SSL/TLS support <code>--with-librist</code> RIST streaming protocol <code>--with-whisper-cpp</code> Whisper speech recognition (alternative to mlx-whisper)"},{"location":"how-to/install-ffmpeg/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/install-ffmpeg/#protocol-not-found-error","title":"\"Protocol not found\" Error","text":"<p>If you see this error when running popup-ai:</p> <pre><code>[ffmpeg] Error opening input: Protocol not found\n[ffmpeg] Error opening input file srt://0.0.0.0:9998?mode=listener&amp;latency=200000\n</code></pre> <p>Your ffmpeg does not have SRT support. Reinstall using the instructions above.</p>"},{"location":"how-to/install-ffmpeg/#brew-conflicts","title":"Brew Conflicts","text":"<p>If you get conflicts with the standard ffmpeg:</p> <pre><code># Remove the standard ffmpeg completely\nbrew uninstall ffmpeg\n\n# Then install the custom build\nbrew install homebrew-ffmpeg/ffmpeg/ffmpeg --with-srt\n</code></pre>"},{"location":"how-to/install-ffmpeg/#build-failures","title":"Build Failures","text":"<p>If the build fails, ensure you have Xcode command line tools:</p> <pre><code>xcode-select --install\n</code></pre> <p>And accept the Xcode license:</p> <pre><code>sudo xcodebuild -license accept\n</code></pre>"},{"location":"how-to/install-ffmpeg/#linux-installation","title":"Linux Installation","text":"<p>On Ubuntu/Debian, ffmpeg typically includes SRT support:</p> <pre><code>sudo apt update\nsudo apt install ffmpeg libsrt-dev\n</code></pre> <p>Verify:</p> <pre><code>ffmpeg -protocols 2&gt;&amp;1 | grep srt\n</code></pre> <p>If SRT is missing, you may need to build from source with <code>--enable-libsrt</code>.</p>"},{"location":"how-to/install-ffmpeg/#windows-installation","title":"Windows Installation","text":"<p>Use the full build from gyan.dev which includes SRT:</p> <ol> <li>Download from: https://www.gyan.dev/ffmpeg/builds/</li> <li>Choose the \"full\" build (not essentials)</li> <li>Extract and add to PATH</li> </ol> <p>Or use winget:</p> <pre><code>winget install ffmpeg\n</code></pre>"},{"location":"how-to/install-ffmpeg/#see-also","title":"See Also","text":"<ul> <li>Quickstart - Get started with popup-ai</li> <li>Configure OBS - Set up OBS for SRT streaming</li> <li>Architecture - How the audio pipeline works</li> </ul>"},{"location":"how-to/pipeline-subsets/","title":"Run Pipeline Subsets","text":"<p>Run only the pipeline stages you need for specific workflows.</p>"},{"location":"how-to/pipeline-subsets/#understanding-actors","title":"Understanding Actors","text":"<p>popup-ai has four pipeline actors:</p> Actor Function Depends On <code>audio_ingest</code> Captures SRT audio None <code>transcriber</code> Converts audio to text <code>audio_ingest</code> <code>annotator</code> Extracts terms via LLM <code>transcriber</code> <code>overlay</code> Sends to OBS <code>annotator</code>"},{"location":"how-to/pipeline-subsets/#enablingdisabling-actors","title":"Enabling/Disabling Actors","text":""},{"location":"how-to/pipeline-subsets/#using-cli-flags-recommended","title":"Using CLI Flags (Recommended)","text":"<p>The easiest way to control actors is with CLI flags:</p> <pre><code># Enable only specific actors\nuv run popup-ai --actors audio,transcriber\n\n# Disable specific actors (keep others enabled)\nuv run popup-ai --no-overlay\nuv run popup-ai --no-annotator --no-overlay\n</code></pre> <p>Available flags:</p> Flag Description <code>--actors LIST</code> Comma-separated list of actors to enable (disables others) <code>--no-audio</code> Disable audio ingest actor <code>--no-transcriber</code> Disable transcriber actor <code>--no-annotator</code> Disable annotator actor <code>--no-overlay</code> Disable overlay actor"},{"location":"how-to/pipeline-subsets/#using-environment-variables","title":"Using Environment Variables","text":"<p>Each actor also has an enable flag via environment variables:</p> <pre><code># Enable/disable specific actors\nexport POPUP_AUDIO_ENABLED=true\nexport POPUP_TRANSCRIBER_ENABLED=true\nexport POPUP_ANNOTATOR_ENABLED=true\nexport POPUP_OVERLAY_ENABLED=true\n</code></pre> <p>Set to <code>false</code> to disable an actor.</p>"},{"location":"how-to/pipeline-subsets/#common-patterns","title":"Common Patterns","text":""},{"location":"how-to/pipeline-subsets/#audio-transcription-only","title":"Audio + Transcription Only","text":"<p>Run transcription without LLM annotations or OBS overlay:</p> <pre><code># Using CLI flags\nuv run popup-ai --actors audio,transcriber\n\n# Or using --no flags\nuv run popup-ai --no-annotator --no-overlay\n\n# Or using environment variables\nexport POPUP_ANNOTATOR_ENABLED=false\nexport POPUP_OVERLAY_ENABLED=false\nuv run popup-ai\n</code></pre>"},{"location":"how-to/pipeline-subsets/#full-pipeline-except-obs","title":"Full Pipeline Except OBS","text":"<p>Run the full pipeline but skip OBS integration:</p> <pre><code>uv run popup-ai --no-overlay\n</code></pre> <p>Useful when:</p> <ul> <li>Testing transcription/annotation without OBS</li> <li>OBS is on a different machine (future remote support)</li> <li>Developing/debugging</li> </ul>"},{"location":"how-to/pipeline-subsets/#transcription-annotation-no-obs","title":"Transcription + Annotation (No OBS)","text":"<p>Test LLM annotations without OBS:</p> <pre><code>uv run popup-ai --no-overlay\n</code></pre>"},{"location":"how-to/pipeline-subsets/#full-pipeline","title":"Full Pipeline","text":"<p>All actors enabled (default):</p> <pre><code>uv run popup-ai\n</code></pre>"},{"location":"how-to/pipeline-subsets/#transcription-only-no-audio-ingest","title":"Transcription Only (No Audio Ingest)","text":"<p>Not Yet Implemented</p> <p>File-based transcription input is planned for a future release.</p>"},{"location":"how-to/pipeline-subsets/#actor-dependencies","title":"Actor Dependencies","text":"<p>When disabling actors, be aware of dependencies:</p> <pre><code>audio_ingest \u2192 transcriber \u2192 annotator \u2192 overlay\n</code></pre> <ul> <li>Disabling <code>transcriber</code> means <code>annotator</code> gets no input</li> <li>Disabling <code>annotator</code> means <code>overlay</code> gets no input</li> </ul> <p>The pipeline handles this gracefully - downstream actors simply wait for input that never comes.</p>"},{"location":"how-to/pipeline-subsets/#checking-actor-status","title":"Checking Actor Status","text":"<p>In the admin UI, disabled actors show as grey (stopped) in the Actor Status panel.</p> <p>You can verify which actors are running:</p> <pre><code># Check Ray actors\nuv run python -c \"import ray; ray.init(); print(ray.actors())\"\n</code></pre>"},{"location":"how-to/pipeline-subsets/#whats-next","title":"What's Next?","text":"<ul> <li>Configuration Reference - All enable flags</li> <li>Architecture - How actors work together</li> </ul>"},{"location":"how-to/tune-transcription/","title":"Tune Transcription Settings","text":"<p>Optimize transcription quality for your content, language, and hardware.</p>"},{"location":"how-to/tune-transcription/#understanding-the-settings","title":"Understanding the Settings","text":"<p>The transcriber has four main settings:</p> Setting Description Trade-off <code>model</code> Whisper model variant Accuracy vs. speed <code>chunk_length_s</code> Audio chunk size Context vs. latency <code>overlap_s</code> Overlap between chunks Continuity vs. processing <code>language</code> Language code Speed vs. auto-detection"},{"location":"how-to/tune-transcription/#choosing-a-model","title":"Choosing a Model","text":"<p>popup-ai uses mlx-whisper models optimized for Apple Silicon.</p>"},{"location":"how-to/tune-transcription/#available-models","title":"Available Models","text":"Model Size Speed Accuracy Memory <code>whisper-tiny</code> 39M Fastest Basic ~1GB <code>whisper-base</code> 74M Fast Good ~1GB <code>whisper-small</code> 244M Medium Better ~2GB <code>whisper-medium</code> 769M Slow Great ~5GB <code>whisper-large-v3</code> 1.5B Slowest Best ~10GB"},{"location":"how-to/tune-transcription/#setting-the-model","title":"Setting the Model","text":"Environment VariableAdmin UI <pre><code>export POPUP_TRANSCRIBER_MODEL=\"mlx-community/whisper-small-mlx\"\n</code></pre> <ol> <li>Open Settings panel</li> <li>Find \"Transcriber\" section</li> <li>Enter model name in \"Model\" field</li> </ol>"},{"location":"how-to/tune-transcription/#model-recommendations","title":"Model Recommendations","text":"Use Case Recommended Model Testing/development <code>whisper-base</code> General streaming <code>whisper-small</code> Technical content <code>whisper-medium</code> Maximum accuracy <code>whisper-large-v3</code> <p>M3 Ultra Users</p> <p>With 128GB+ RAM, you can comfortably run <code>whisper-large-v3</code> for the best accuracy.</p>"},{"location":"how-to/tune-transcription/#chunk-length","title":"Chunk Length","text":"<p>Chunk length determines how much audio is processed at once.</p>"},{"location":"how-to/tune-transcription/#trade-offs","title":"Trade-offs","text":"Shorter chunks (2-3s) Longer chunks (5-10s) Lower latency Higher latency Less context More context May cut words Better sentence detection More processing overhead More efficient"},{"location":"how-to/tune-transcription/#setting-chunk-length","title":"Setting Chunk Length","text":"<pre><code>export POPUP_TRANSCRIBER_CHUNK_LENGTH_S=\"5.0\"\n</code></pre>"},{"location":"how-to/tune-transcription/#recommendations","title":"Recommendations","text":"Priority Chunk Length Low latency 2.0 - 3.0 Balanced 5.0 Maximum accuracy 8.0 - 10.0"},{"location":"how-to/tune-transcription/#overlap","title":"Overlap","text":"<p>Overlap keeps audio from the end of one chunk to include at the start of the next.</p>"},{"location":"how-to/tune-transcription/#why-overlap-matters","title":"Why Overlap Matters","text":"<p>Without overlap, words at chunk boundaries may be cut:</p> <pre><code>Chunk 1: \"Today we're going to talk about machi-\"\nChunk 2: \"-ne learning and neural networks\"\n</code></pre> <p>With overlap, the boundary is handled smoothly.</p>"},{"location":"how-to/tune-transcription/#setting-overlap","title":"Setting Overlap","text":"<pre><code>export POPUP_TRANSCRIBER_OVERLAP_S=\"0.5\"\n</code></pre>"},{"location":"how-to/tune-transcription/#recommendations_1","title":"Recommendations","text":"Chunk Length Recommended Overlap 2.0s 0.3s 5.0s 0.5s 10.0s 1.0s <p>Rule of thumb: overlap should be ~10% of chunk length.</p>"},{"location":"how-to/tune-transcription/#language-setting","title":"Language Setting","text":"<p>By default, Whisper auto-detects the language. You can specify a language for faster processing.</p>"},{"location":"how-to/tune-transcription/#setting-language","title":"Setting Language","text":"<pre><code># Auto-detect (default)\nexport POPUP_TRANSCRIBER_LANGUAGE=\"\"\n\n# Force English\nexport POPUP_TRANSCRIBER_LANGUAGE=\"en\"\n\n# Force Spanish\nexport POPUP_TRANSCRIBER_LANGUAGE=\"es\"\n</code></pre>"},{"location":"how-to/tune-transcription/#when-to-set-language","title":"When to Set Language","text":"<ul> <li>Set it if you always stream in one language</li> <li>Leave auto if you switch languages or have multilingual content</li> </ul> <p>Setting the language:</p> <ul> <li>Skips language detection step</li> <li>Slightly faster processing</li> <li>May improve accuracy for that language</li> </ul>"},{"location":"how-to/tune-transcription/#tuning-workflow","title":"Tuning Workflow","text":""},{"location":"how-to/tune-transcription/#step-1-start-with-defaults","title":"Step 1: Start with Defaults","text":"<pre><code># Default settings\nPOPUP_TRANSCRIBER_MODEL=\"mlx-community/whisper-base-mlx\"\nPOPUP_TRANSCRIBER_CHUNK_LENGTH_S=\"5.0\"\nPOPUP_TRANSCRIBER_OVERLAP_S=\"0.5\"\nPOPUP_TRANSCRIBER_LANGUAGE=\"\"\n</code></pre>"},{"location":"how-to/tune-transcription/#step-2-monitor-quality","title":"Step 2: Monitor Quality","text":"<ol> <li>Start popup-ai with defaults</li> <li>Stream some representative content</li> <li>Watch the Live Transcript panel</li> <li>Note any issues:</li> <li>Words cut off at boundaries?</li> <li>Wrong language detected?</li> <li>Missing technical terms?</li> <li>Too slow/delayed?</li> </ol>"},{"location":"how-to/tune-transcription/#step-3-adjust-one-setting","title":"Step 3: Adjust One Setting","text":"<p>Change only one setting at a time:</p> <ol> <li>Accuracy issues \u2192 Try larger model</li> <li>Words cut off \u2192 Increase overlap</li> <li>Too slow \u2192 Reduce chunk length or use smaller model</li> <li>Wrong language \u2192 Set explicit language</li> </ol>"},{"location":"how-to/tune-transcription/#step-4-test-and-iterate","title":"Step 4: Test and Iterate","text":"<p>Repeat with your actual streaming content until quality is acceptable.</p>"},{"location":"how-to/tune-transcription/#performance-considerations","title":"Performance Considerations","text":""},{"location":"how-to/tune-transcription/#memory-usage","title":"Memory Usage","text":"<p>Monitor memory when using larger models:</p> <pre><code># Check memory usage\ntop -l 1 | grep -E \"PhysMem\"\n</code></pre>"},{"location":"how-to/tune-transcription/#cpu-usage","title":"CPU Usage","text":"<p>Larger models and shorter chunks increase CPU load. Monitor with:</p> <pre><code># Check Ray worker CPU usage\ntop -l 1 | grep -E \"ray\"\n</code></pre>"},{"location":"how-to/tune-transcription/#latency-measurement","title":"Latency Measurement","text":"<p>The admin UI shows stats for latency estimation:</p> <ul> <li><code>audio_seconds_processed</code> - How much audio has been transcribed</li> <li><code>buffer_duration_s</code> - Current audio buffer size</li> </ul> <p>If buffer keeps growing, processing is falling behind.</p>"},{"location":"how-to/tune-transcription/#all-settings-reference","title":"All Settings Reference","text":"<pre><code># Model selection\nPOPUP_TRANSCRIBER_MODEL=\"mlx-community/whisper-small-mlx\"\n\n# Chunk settings\nPOPUP_TRANSCRIBER_CHUNK_LENGTH_S=\"5.0\"\nPOPUP_TRANSCRIBER_OVERLAP_S=\"0.5\"\n\n# Language (empty for auto-detect)\nPOPUP_TRANSCRIBER_LANGUAGE=\"en\"\n</code></pre>"},{"location":"how-to/tune-transcription/#whats-next","title":"What's Next?","text":"<ul> <li>Configuration Reference - All settings</li> <li>Architecture - How transcription fits in</li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>Reference documentation provides technical descriptions of the CLI, configuration, and internals. Use this when you need to know exactly what options are available.</p>"},{"location":"reference/#available-reference","title":"Available Reference","text":""},{"location":"reference/#cli-commands","title":"CLI Commands","text":"<p>Complete command-line interface reference.</p> <ul> <li>All commands and subcommands</li> <li>Options and flags</li> <li>Environment variables</li> <li>Exit codes</li> </ul>"},{"location":"reference/#configuration","title":"Configuration","text":"<p>All configuration options with defaults.</p> <ul> <li>Audio ingest settings</li> <li>Transcriber settings</li> <li>Annotator settings</li> <li>Overlay settings</li> <li>Pipeline settings</li> </ul>"},{"location":"reference/#actors","title":"Actors","text":"<p>Technical reference for Ray actors.</p> <ul> <li>Actor responsibilities</li> <li>Message types</li> <li>Queue topology</li> <li>Status and health checks</li> </ul>"},{"location":"reference/actors/","title":"Actors Reference","text":"<p>Technical reference for popup-ai's Ray actors.</p>"},{"location":"reference/actors/#overview","title":"Overview","text":"<p>popup-ai uses Ray actors as the backbone of its pipeline architecture. Each pipeline stage is an isolated actor that communicates via Ray Queues.</p>"},{"location":"reference/actors/#actor-hierarchy","title":"Actor Hierarchy","text":"<pre><code>PipelineSupervisor\n\u251c\u2500\u2500 AudioIngestActor\n\u251c\u2500\u2500 TranscriberActor\n\u251c\u2500\u2500 AnnotatorActor\n\u2514\u2500\u2500 OverlayActor\n</code></pre>"},{"location":"reference/actors/#pipelinesupervisor","title":"PipelineSupervisor","text":"<p>Module: <code>popup_ai.actors.supervisor</code></p> <p>The supervisor manages all child actor lifecycles.</p>"},{"location":"reference/actors/#responsibilities","title":"Responsibilities","text":"<ul> <li>Spawn and configure child actors</li> <li>Health monitoring (every 5 seconds)</li> <li>Automatic restart on actor crash</li> <li>Graceful shutdown coordination</li> <li>Queue creation and management</li> </ul>"},{"location":"reference/actors/#methods","title":"Methods","text":"Method Description <code>start()</code> Start all enabled actors <code>stop()</code> Stop all actors gracefully <code>get_status()</code> Get status of all actors <code>start_actor(name)</code> Start a specific actor <code>stop_actor(name)</code> Stop a specific actor <code>restart_actor(name)</code> Restart a specific actor <code>configure(settings)</code> Update configuration"},{"location":"reference/actors/#queues-created","title":"Queues Created","text":"Queue Capacity Flow <code>audio</code> 100 AudioIngest \u2192 Transcriber <code>transcript</code> 100 Transcriber \u2192 Annotator <code>annotation</code> 100 Annotator \u2192 Overlay <code>ui</code> 1000 All actors \u2192 UI"},{"location":"reference/actors/#audioingestactor","title":"AudioIngestActor","text":"<p>Module: <code>popup_ai.actors.audio_ingest</code></p> <p>Captures audio from SRT stream via ffmpeg subprocess.</p>"},{"location":"reference/actors/#configuration","title":"Configuration","text":"<p>Uses <code>AudioIngestConfig</code>:</p> <ul> <li><code>srt_port</code>: SRT listener port</li> <li><code>srt_latency_ms</code>: SRT latency</li> <li><code>sample_rate</code>: Audio sample rate (Hz)</li> <li><code>channels</code>: Number of channels</li> <li><code>chunk_duration_ms</code>: Chunk size</li> </ul>"},{"location":"reference/actors/#data-flow","title":"Data Flow","text":"<pre><code>SRT Stream \u2192 ffmpeg subprocess \u2192 PCM data \u2192 AudioChunk \u2192 audio_queue\n</code></pre>"},{"location":"reference/actors/#stats","title":"Stats","text":"Stat Description <code>chunks_sent</code> Number of audio chunks sent <code>bytes_processed</code> Total bytes processed <code>uptime_s</code> Actor uptime in seconds"},{"location":"reference/actors/#ffmpeg-command","title":"ffmpeg Command","text":"<pre><code>ffmpeg -hide_banner -loglevel warning \\\n  -i \"srt://0.0.0.0:{port}?mode=listener&amp;latency={latency}\" \\\n  -vn -acodec pcm_s16le -ar {sample_rate} -ac {channels} \\\n  -f s16le pipe:1\n</code></pre>"},{"location":"reference/actors/#transcriberactor","title":"TranscriberActor","text":"<p>Module: <code>popup_ai.actors.transcriber</code></p> <p>Transcribes audio using mlx-whisper.</p>"},{"location":"reference/actors/#configuration_1","title":"Configuration","text":"<p>Uses <code>TranscriberConfig</code>:</p> <ul> <li><code>model</code>: Whisper model identifier</li> <li><code>chunk_length_s</code>: Audio chunk length</li> <li><code>overlap_s</code>: Overlap between chunks</li> <li><code>language</code>: Language code or None</li> </ul>"},{"location":"reference/actors/#data-flow_1","title":"Data Flow","text":"<pre><code>audio_queue \u2192 AudioChunk \u2192 buffer \u2192 WAV file \u2192 mlx-whisper \u2192 Transcript \u2192 transcript_queue\n</code></pre>"},{"location":"reference/actors/#processing-logic","title":"Processing Logic","text":"<ol> <li>Accumulate AudioChunks in buffer</li> <li>When buffer &gt;= <code>chunk_length_s</code>:</li> <li>Write buffer to temp WAV file</li> <li>Run mlx-whisper transcription</li> <li>Keep <code>overlap_s</code> of audio for next chunk</li> <li>Push Transcript to output queue</li> </ol>"},{"location":"reference/actors/#stats_1","title":"Stats","text":"Stat Description <code>transcripts_sent</code> Number of transcripts sent <code>audio_seconds_processed</code> Total audio processed <code>buffer_duration_s</code> Current buffer size <code>uptime_s</code> Actor uptime in seconds"},{"location":"reference/actors/#annotatoractor","title":"AnnotatorActor","text":"<p>Module: <code>popup_ai.actors.annotator</code></p> <p>Extracts terms and generates explanations via LLM.</p>"},{"location":"reference/actors/#configuration_2","title":"Configuration","text":"<p>Uses <code>AnnotatorConfig</code>:</p> <ul> <li><code>provider</code>: LLM provider</li> <li><code>model</code>: LLM model</li> <li><code>cache_enabled</code>: Enable SQLite cache</li> <li><code>cache_path</code>: Cache database path</li> <li><code>prompt_template</code>: Prompt template</li> </ul>"},{"location":"reference/actors/#data-flow_2","title":"Data Flow","text":"<pre><code>transcript_queue \u2192 Transcript \u2192 cache check \u2192 LLM call \u2192 Annotation \u2192 annotation_queue\n</code></pre>"},{"location":"reference/actors/#cache-schema","title":"Cache Schema","text":"<pre><code>CREATE TABLE annotations (\n    text_hash TEXT PRIMARY KEY,\n    term TEXT NOT NULL,\n    explanation TEXT NOT NULL,\n    created_at REAL NOT NULL\n)\n</code></pre>"},{"location":"reference/actors/#stats_2","title":"Stats","text":"Stat Description <code>annotations_sent</code> Number of annotations sent <code>cache_hits</code> Cache hits <code>llm_calls</code> LLM API calls made <code>uptime_s</code> Actor uptime in seconds"},{"location":"reference/actors/#overlayactor","title":"OverlayActor","text":"<p>Module: <code>popup_ai.actors.overlay</code></p> <p>Sends annotations to OBS via WebSocket.</p>"},{"location":"reference/actors/#configuration_3","title":"Configuration","text":"<p>Uses <code>OverlayConfig</code>:</p> <ul> <li><code>obs_host</code>: OBS WebSocket host</li> <li><code>obs_port</code>: OBS WebSocket port</li> <li><code>obs_password</code>: OBS WebSocket password</li> <li><code>scene_name</code>: Target OBS scene</li> <li><code>hold_duration_ms</code>: Display duration</li> <li><code>max_slots</code>: Number of overlay slots</li> </ul>"},{"location":"reference/actors/#data-flow_3","title":"Data Flow","text":"<pre><code>annotation_queue \u2192 Annotation \u2192 slot assignment \u2192 OBS WebSocket \u2192 text source update\n</code></pre>"},{"location":"reference/actors/#slot-management","title":"Slot Management","text":"<ul> <li>4 slots available (configurable via <code>max_slots</code>)</li> <li>Round-robin assignment</li> <li>Auto-hide after <code>hold_duration_ms</code></li> <li>Cleanup loop runs every 500ms</li> </ul>"},{"location":"reference/actors/#obs-sources","title":"OBS Sources","text":"<p>Creates text sources named:</p> <ul> <li><code>popup-ai-slot-1</code></li> <li><code>popup-ai-slot-2</code></li> <li><code>popup-ai-slot-3</code></li> <li><code>popup-ai-slot-4</code></li> </ul>"},{"location":"reference/actors/#stats_3","title":"Stats","text":"Stat Description <code>annotations_displayed</code> Total annotations shown <code>obs_connected</code> OBS connection status <code>active_slots</code> Currently active slots <code>uptime_s</code> Actor uptime in seconds"},{"location":"reference/actors/#message-types","title":"Message Types","text":"<p>Defined in <code>popup_ai.messages</code>:</p>"},{"location":"reference/actors/#audiochunk","title":"AudioChunk","text":"<pre><code>class AudioChunk(BaseModel):\n    data: bytes           # Raw PCM audio data\n    timestamp_ms: int     # Capture timestamp\n    sample_rate: int      # Sample rate (default: 16000)\n    channels: int         # Channels (default: 1)\n</code></pre>"},{"location":"reference/actors/#transcript","title":"Transcript","text":"<pre><code>class Transcript(BaseModel):\n    text: str                           # Full transcript text\n    segments: list[TranscriptSegment]   # Word-level segments\n    is_partial: bool                    # Partial vs final\n    timestamp_ms: int                   # Processing timestamp\n</code></pre>"},{"location":"reference/actors/#annotation","title":"Annotation","text":"<pre><code>class Annotation(BaseModel):\n    term: str                # Extracted term\n    explanation: str         # Brief explanation\n    display_duration_ms: int # Display time (default: 5000)\n    slot: int               # Overlay slot (1-4)\n    timestamp_ms: int       # Generation timestamp\n</code></pre>"},{"location":"reference/actors/#uievent","title":"UIEvent","text":"<pre><code>class UIEvent(BaseModel):\n    source: str      # Actor name\n    event_type: str  # Event type (started, stopped, error, etc.)\n    data: dict       # Event-specific data\n    timestamp_ms: int\n</code></pre>"},{"location":"reference/actors/#actorstatus","title":"ActorStatus","text":"<pre><code>class ActorStatus(BaseModel):\n    name: str           # Actor name\n    state: str          # stopped, starting, running, error\n    error: str | None   # Error message if state is error\n    stats: dict         # Actor-specific statistics\n</code></pre>"},{"location":"reference/actors/#health-checks","title":"Health Checks","text":"<p>Each actor implements <code>health_check() -&gt; bool</code>:</p> <ul> <li>Returns <code>True</code> if actor is healthy</li> <li>Supervisor checks every 5 seconds</li> <li>Unhealthy actors are automatically restarted</li> </ul>"},{"location":"reference/actors/#see-also","title":"See Also","text":"<ul> <li>Architecture - Design overview</li> <li>Ray Actors - Why Ray?</li> </ul>"},{"location":"reference/cli/","title":"CLI Reference","text":"<p>Complete command-line interface reference for popup-ai.</p>"},{"location":"reference/cli/#synopsis","title":"Synopsis","text":"<pre><code>popup-ai [OPTIONS] [COMMAND]\n</code></pre>"},{"location":"reference/cli/#global-options","title":"Global Options","text":"Option Short Description <code>--version</code> <code>-v</code> Show version and exit <code>--headless</code> Run without UI <code>--config PATH</code> <code>-c</code> Path to config file <code>--actors LIST</code> Comma-separated list of actors to enable <code>--no-audio</code> Disable audio ingest actor <code>--no-transcriber</code> Disable transcriber actor <code>--no-annotator</code> Disable annotator actor <code>--no-overlay</code> Disable overlay actor <code>--help</code> Show help and exit"},{"location":"reference/cli/#commands","title":"Commands","text":""},{"location":"reference/cli/#default-no-command","title":"Default (no command)","text":"<p>Starts popup-ai with the admin UI.</p> <pre><code>popup-ai\n</code></pre> <p>Opens browser to <code>http://127.0.0.1:8080</code>.</p>"},{"location":"reference/cli/#status","title":"status","text":"<p>Shows pipeline status.</p> <pre><code>popup-ai status\n</code></pre> <p>Displays whether Ray is initialized and pipeline state.</p>"},{"location":"reference/cli/#options-detail","title":"Options Detail","text":""},{"location":"reference/cli/#-version-v","title":"--version, -v","text":"<p>Print version information and exit.</p> <pre><code>$ popup-ai --version\npopup-ai version 0.1.0\n</code></pre>"},{"location":"reference/cli/#-headless","title":"--headless","text":"<p>Run without the NiceGUI admin UI. Useful for:</p> <ul> <li>Automation and scripting</li> <li>Running on headless servers</li> <li>CI/CD pipelines</li> </ul> <pre><code>popup-ai --headless\n</code></pre> <p>The pipeline starts immediately. Press Ctrl+C to stop.</p>"},{"location":"reference/cli/#-config-c","title":"--config, -c","text":"<p>Specify a configuration file path.</p> <pre><code>popup-ai --config /path/to/config.toml\n</code></pre> <p>Config File Support</p> <p>Config file loading is planned but not yet implemented. Use environment variables for configuration.</p>"},{"location":"reference/cli/#-actors","title":"--actors","text":"<p>Enable only specific actors. Accepts a comma-separated list.</p> <p>Valid actors: <code>audio</code>, <code>transcriber</code>, <code>annotator</code>, <code>overlay</code></p> <pre><code># Only audio + transcription\npopup-ai --actors audio,transcriber\n\n# Only transcription + annotation (no audio or overlay)\npopup-ai --actors transcriber,annotator\n</code></pre> <p>When <code>--actors</code> is specified, all actors not in the list are disabled.</p>"},{"location":"reference/cli/#-no-audio-no-transcriber-no-annotator-no-overlay","title":"--no-audio, --no-transcriber, --no-annotator, --no-overlay","text":"<p>Disable specific actors while keeping others enabled.</p> <pre><code># Full pipeline except OBS overlay\npopup-ai --no-overlay\n\n# Audio + transcription only\npopup-ai --no-annotator --no-overlay\n</code></pre> <p>These flags are mutually exclusive with <code>--actors</code>. If both are specified, <code>--actors</code> takes precedence.</p>"},{"location":"reference/cli/#environment-variables","title":"Environment Variables","text":"<p>All configuration can be set via environment variables with the <code>POPUP_</code> prefix.</p>"},{"location":"reference/cli/#pipeline-settings","title":"Pipeline Settings","text":"Variable Default Description <code>POPUP_AUDIO_ENABLED</code> <code>true</code> Enable audio ingest actor <code>POPUP_TRANSCRIBER_ENABLED</code> <code>true</code> Enable transcriber actor <code>POPUP_ANNOTATOR_ENABLED</code> <code>true</code> Enable annotator actor <code>POPUP_OVERLAY_ENABLED</code> <code>true</code> Enable overlay actor <code>POPUP_HEADLESS</code> <code>false</code> Run without UI <code>POPUP_LOG_LEVEL</code> <code>INFO</code> Logging level"},{"location":"reference/cli/#audio-settings","title":"Audio Settings","text":"Variable Default Description <code>POPUP_AUDIO_SRT_PORT</code> <code>9998</code> SRT listener port <code>POPUP_AUDIO_SRT_LATENCY_MS</code> <code>200</code> SRT latency in ms <code>POPUP_AUDIO_SAMPLE_RATE</code> <code>16000</code> Audio sample rate <code>POPUP_AUDIO_CHANNELS</code> <code>1</code> Audio channels <code>POPUP_AUDIO_CHUNK_DURATION_MS</code> <code>100</code> Chunk duration"},{"location":"reference/cli/#transcriber-settings","title":"Transcriber Settings","text":"Variable Default Description <code>POPUP_TRANSCRIBER_MODEL</code> <code>mlx-community/whisper-base-mlx</code> Whisper model <code>POPUP_TRANSCRIBER_CHUNK_LENGTH_S</code> <code>5.0</code> Chunk length <code>POPUP_TRANSCRIBER_OVERLAP_S</code> <code>0.5</code> Chunk overlap <code>POPUP_TRANSCRIBER_LANGUAGE</code> (auto) Language code"},{"location":"reference/cli/#annotator-settings","title":"Annotator Settings","text":"Variable Default Description <code>POPUP_ANNOTATOR_PROVIDER</code> <code>openai</code> LLM provider <code>POPUP_ANNOTATOR_MODEL</code> <code>gpt-4o-mini</code> LLM model <code>POPUP_ANNOTATOR_CACHE_ENABLED</code> <code>true</code> Enable SQLite cache <code>POPUP_ANNOTATOR_CACHE_PATH</code> <code>~/.popup-ai/cache.db</code> Cache path"},{"location":"reference/cli/#overlay-settings","title":"Overlay Settings","text":"Variable Default Description <code>POPUP_OVERLAY_OBS_HOST</code> <code>localhost</code> OBS WebSocket host <code>POPUP_OVERLAY_OBS_PORT</code> <code>4455</code> OBS WebSocket port <code>POPUP_OVERLAY_OBS_PASSWORD</code> (none) OBS WebSocket password <code>POPUP_OVERLAY_SCENE_NAME</code> <code>popup-ai-overlay</code> OBS scene name <code>POPUP_OVERLAY_HOLD_DURATION_MS</code> <code>5000</code> Annotation display time <code>POPUP_OVERLAY_MAX_SLOTS</code> <code>4</code> Number of overlay slots"},{"location":"reference/cli/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 Error (missing dependency, configuration error)"},{"location":"reference/cli/#examples","title":"Examples","text":""},{"location":"reference/cli/#start-with-ui-default","title":"Start with UI (default)","text":"<pre><code>popup-ai\n</code></pre>"},{"location":"reference/cli/#start-headless","title":"Start headless","text":"<pre><code>popup-ai --headless\n</code></pre>"},{"location":"reference/cli/#start-with-custom-port","title":"Start with custom port","text":"<pre><code>POPUP_AUDIO_SRT_PORT=9999 popup-ai\n</code></pre>"},{"location":"reference/cli/#start-with-specific-model","title":"Start with specific model","text":"<pre><code>POPUP_TRANSCRIBER_MODEL=\"mlx-community/whisper-small-mlx\" popup-ai\n</code></pre>"},{"location":"reference/cli/#disable-overlay-for-testing","title":"Disable overlay for testing","text":"<pre><code># Using CLI flag (recommended)\npopup-ai --no-overlay\n\n# Using environment variable\nPOPUP_OVERLAY_ENABLED=false popup-ai\n</code></pre>"},{"location":"reference/cli/#run-only-audio-transcription","title":"Run only audio + transcription","text":"<pre><code>popup-ai --actors audio,transcriber\n</code></pre>"},{"location":"reference/cli/#run-headless-without-overlay","title":"Run headless without overlay","text":"<pre><code>popup-ai --headless --no-overlay\n</code></pre>"},{"location":"reference/cli/#see-also","title":"See Also","text":"<ul> <li>Configuration Reference - Detailed configuration options</li> <li>Quickstart Tutorial - Getting started</li> </ul>"},{"location":"reference/configuration/","title":"Configuration Reference","text":"<p>Complete configuration reference for popup-ai.</p>"},{"location":"reference/configuration/#overview","title":"Overview","text":"<p>popup-ai uses pydantic-settings for configuration. Settings are loaded from:</p> <ol> <li>Default values (in code)</li> <li>Environment variables (with <code>POPUP_</code> prefix)</li> <li>Config file (planned, not yet implemented)</li> </ol>"},{"location":"reference/configuration/#configuration-sections","title":"Configuration Sections","text":""},{"location":"reference/configuration/#pipelineconfig","title":"PipelineConfig","text":"<p>Controls which actors are enabled and global settings.</p> Setting Env Var Type Default Description <code>audio_enabled</code> <code>POPUP_AUDIO_ENABLED</code> bool <code>true</code> Enable audio ingest actor <code>transcriber_enabled</code> <code>POPUP_TRANSCRIBER_ENABLED</code> bool <code>true</code> Enable transcriber actor <code>annotator_enabled</code> <code>POPUP_ANNOTATOR_ENABLED</code> bool <code>true</code> Enable annotator actor <code>overlay_enabled</code> <code>POPUP_OVERLAY_ENABLED</code> bool <code>true</code> Enable overlay actor <code>headless</code> <code>POPUP_HEADLESS</code> bool <code>false</code> Run without admin UI <code>log_level</code> <code>POPUP_LOG_LEVEL</code> str <code>INFO</code> Logging level"},{"location":"reference/configuration/#audioingestconfig","title":"AudioIngestConfig","text":"<p>Settings for the SRT audio capture actor.</p> Setting Env Var Type Default Description <code>srt_port</code> <code>POPUP_AUDIO_SRT_PORT</code> int <code>9998</code> Port for SRT listener <code>srt_latency_ms</code> <code>POPUP_AUDIO_SRT_LATENCY_MS</code> int <code>200</code> SRT buffer latency in milliseconds <code>sample_rate</code> <code>POPUP_AUDIO_SAMPLE_RATE</code> int <code>16000</code> Audio sample rate in Hz <code>channels</code> <code>POPUP_AUDIO_CHANNELS</code> int <code>1</code> Number of audio channels <code>chunk_duration_ms</code> <code>POPUP_AUDIO_CHUNK_DURATION_MS</code> int <code>100</code> Duration of each audio chunk in ms"},{"location":"reference/configuration/#notes","title":"Notes","text":"<ul> <li><code>srt_port</code>: Must not conflict with other services. Default 9998 is typically available.</li> <li><code>srt_latency_ms</code>: Higher values provide more buffer against network jitter but add delay. Range: 120-500ms.</li> <li><code>sample_rate</code>: 16000 Hz is optimal for Whisper. Other values may affect quality.</li> <li><code>channels</code>: Mono (1) is recommended. Stereo will be downmixed.</li> </ul>"},{"location":"reference/configuration/#transcriberconfig","title":"TranscriberConfig","text":"<p>Settings for the mlx-whisper transcription actor.</p> Setting Env Var Type Default Description <code>model</code> <code>POPUP_TRANSCRIBER_MODEL</code> str <code>mlx-community/whisper-base-mlx</code> Whisper model identifier <code>chunk_length_s</code> <code>POPUP_TRANSCRIBER_CHUNK_LENGTH_S</code> float <code>5.0</code> Audio chunk length for processing <code>overlap_s</code> <code>POPUP_TRANSCRIBER_OVERLAP_S</code> float <code>0.5</code> Overlap between consecutive chunks <code>language</code> <code>POPUP_TRANSCRIBER_LANGUAGE</code> str|None <code>None</code> Language code (e.g., \"en\") or None for auto"},{"location":"reference/configuration/#model-options","title":"Model Options","text":"Model HuggingFace ID Size Accuracy Tiny <code>mlx-community/whisper-tiny-mlx</code> 39M Basic Base <code>mlx-community/whisper-base-mlx</code> 74M Good Small <code>mlx-community/whisper-small-mlx</code> 244M Better Medium <code>mlx-community/whisper-medium-mlx</code> 769M Great Large <code>mlx-community/whisper-large-v3-mlx</code> 1.5B Best"},{"location":"reference/configuration/#annotatorconfig","title":"AnnotatorConfig","text":"<p>Settings for the LLM annotation actor.</p> Setting Env Var Type Default Description <code>provider</code> <code>POPUP_ANNOTATOR_PROVIDER</code> str <code>openai</code> LLM provider name <code>model</code> <code>POPUP_ANNOTATOR_MODEL</code> str <code>gpt-4o-mini</code> LLM model name <code>cache_enabled</code> <code>POPUP_ANNOTATOR_CACHE_ENABLED</code> bool <code>true</code> Enable SQLite result cache <code>cache_path</code> <code>POPUP_ANNOTATOR_CACHE_PATH</code> Path <code>~/.popup-ai/cache.db</code> Path to cache database <code>prompt_template</code> <code>POPUP_ANNOTATOR_PROMPT_TEMPLATE</code> str (see below) Prompt template"},{"location":"reference/configuration/#default-prompt-template","title":"Default Prompt Template","text":"<pre><code>Extract key terms and provide brief explanations for: {text}\n</code></pre>"},{"location":"reference/configuration/#provider-configuration","title":"Provider Configuration","text":"<p>The annotator uses pydantic-ai. Configure provider credentials via standard environment variables:</p> Provider Credential Variable OpenAI <code>OPENAI_API_KEY</code> Anthropic <code>ANTHROPIC_API_KEY</code> Google <code>GOOGLE_API_KEY</code>"},{"location":"reference/configuration/#overlayconfig","title":"OverlayConfig","text":"<p>Settings for the OBS WebSocket overlay actor.</p> Setting Env Var Type Default Description <code>obs_host</code> <code>POPUP_OVERLAY_OBS_HOST</code> str <code>localhost</code> OBS WebSocket host <code>obs_port</code> <code>POPUP_OVERLAY_OBS_PORT</code> int <code>4455</code> OBS WebSocket port <code>obs_password</code> <code>POPUP_OVERLAY_OBS_PASSWORD</code> str|None <code>None</code> OBS WebSocket password <code>scene_name</code> <code>POPUP_OVERLAY_SCENE_NAME</code> str <code>popup-ai-overlay</code> OBS scene for overlays <code>hold_duration_ms</code> <code>POPUP_OVERLAY_HOLD_DURATION_MS</code> int <code>5000</code> How long to display each annotation <code>max_slots</code> <code>POPUP_OVERLAY_MAX_SLOTS</code> int <code>4</code> Number of overlay slots"},{"location":"reference/configuration/#obs-websocket-setup","title":"OBS WebSocket Setup","text":"<ol> <li>In OBS, go to Tools \u2192 WebSocket Server Settings</li> <li>Enable the WebSocket server</li> <li>Note the port (default: 4455)</li> <li>Set a password if desired</li> <li>Configure popup-ai with matching settings</li> </ol>"},{"location":"reference/configuration/#example-configurations","title":"Example Configurations","text":""},{"location":"reference/configuration/#development-setup","title":"Development Setup","text":"<p>Minimal configuration for local development:</p> <pre><code>export POPUP_OVERLAY_ENABLED=false\nexport POPUP_TRANSCRIBER_MODEL=\"mlx-community/whisper-tiny-mlx\"\n</code></pre>"},{"location":"reference/configuration/#production-setup","title":"Production Setup","text":"<p>Full pipeline with optimized settings:</p> <pre><code>export POPUP_TRANSCRIBER_MODEL=\"mlx-community/whisper-small-mlx\"\nexport POPUP_TRANSCRIBER_CHUNK_LENGTH_S=\"5.0\"\nexport POPUP_AUDIO_SRT_LATENCY_MS=\"200\"\nexport POPUP_OVERLAY_HOLD_DURATION_MS=\"6000\"\nexport POPUP_LOG_LEVEL=\"WARNING\"\n</code></pre>"},{"location":"reference/configuration/#remote-obs","title":"Remote OBS","text":"<p>Connecting to OBS on another machine:</p> <pre><code>export POPUP_OVERLAY_OBS_HOST=\"192.168.1.100\"\nexport POPUP_OVERLAY_OBS_PORT=\"4455\"\nexport POPUP_OVERLAY_OBS_PASSWORD=\"your-password\"\n</code></pre>"},{"location":"reference/configuration/#see-also","title":"See Also","text":"<ul> <li>CLI Reference - Command-line options</li> <li>Tune Transcription - Optimizing transcription</li> <li>Configure OBS - OBS setup guide</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Tutorials are learning-oriented lessons that take you through a series of steps to complete a project. They help you get started and build understanding through doing.</p>"},{"location":"tutorials/#available-tutorials","title":"Available Tutorials","text":""},{"location":"tutorials/#quickstart","title":"Quickstart","text":"<p>Get popup-ai running and see your first annotation in about 5 minutes. You'll install the package, start the app, connect OBS, and watch annotations appear.</p> <p>What you'll learn:</p> <ul> <li>Installing popup-ai</li> <li>Starting the admin UI</li> <li>Basic OBS configuration for SRT</li> <li>Starting the pipeline</li> </ul>"},{"location":"tutorials/#understanding-the-admin-ui","title":"Understanding the Admin UI","text":"<p>A guided tour of the admin interface. Learn what each panel shows, how to interpret actor status, and how to use the controls.</p> <p>What you'll learn:</p> <ul> <li>Actor status indicators</li> <li>Pipeline controls</li> <li>Live transcript view</li> <li>Settings configuration</li> </ul>"},{"location":"tutorials/#prerequisites","title":"Prerequisites","text":"<p>Before starting these tutorials, ensure you have:</p> <ul> <li>macOS with Apple Silicon (M1/M2/M3)</li> <li>Python 3.13+ installed</li> <li>OBS Studio installed</li> <li>ffmpeg installed (<code>brew install ffmpeg</code>)</li> </ul>"},{"location":"tutorials/admin-ui/","title":"Understanding the Admin UI","text":"<p>A guided tour of the popup-ai admin interface. Learn what each panel shows and how to use the controls effectively.</p>"},{"location":"tutorials/admin-ui/#launching-the-ui","title":"Launching the UI","text":"<p>Start popup-ai to open the admin UI:</p> <pre><code>uv run popup-ai\n</code></pre> <p>The UI opens at <code>http://127.0.0.1:8080</code>.</p> <p>Headless Mode</p> <p>If you don't need the UI, run <code>popup-ai --headless</code> for automation scenarios.</p>"},{"location":"tutorials/admin-ui/#ui-overview","title":"UI Overview","text":"<p>The admin UI has four main areas:</p> <ol> <li>Header - Pipeline controls</li> <li>Actor Status - Health monitoring</li> <li>Live Transcript - Real-time transcription output</li> <li>Recent Annotations - Generated annotations</li> <li>Settings - Configuration panel</li> </ol>"},{"location":"tutorials/admin-ui/#header-and-pipeline-controls","title":"Header and Pipeline Controls","text":"<p>The header contains the main pipeline controls:</p> Button Action Start Pipeline Spawns all actors and begins processing Stop Pipeline Gracefully stops all actors <p>When the pipeline is stopped, only \"Start Pipeline\" is enabled. Once running, only \"Stop Pipeline\" is enabled.</p>"},{"location":"tutorials/admin-ui/#actor-status-panel","title":"Actor Status Panel","text":"<p>The left panel shows the status of each pipeline actor:</p>"},{"location":"tutorials/admin-ui/#status-indicators","title":"Status Indicators","text":"Icon Meaning :material-check-circle:{ style=\"color: green\" } Running normally :material-clock:{ style=\"color: orange\" } Starting up :material-alert-circle:{ style=\"color: red\" } Error state :material-circle:{ style=\"color: grey\" } Stopped"},{"location":"tutorials/admin-ui/#actors","title":"Actors","text":"<ul> <li>Audio Ingest - Captures audio from SRT stream via ffmpeg</li> <li>Transcriber - Converts audio to text using mlx-whisper</li> <li>Annotator - Extracts terms and generates explanations via LLM</li> <li>Overlay - Sends annotations to OBS for display</li> </ul>"},{"location":"tutorials/admin-ui/#stats","title":"Stats","text":"<p>Each actor card shows relevant statistics:</p> <ul> <li>Audio Ingest: chunks_sent, bytes_processed</li> <li>Transcriber: transcripts_sent, audio_seconds_processed</li> <li>Annotator: annotations_sent, cache_hits, llm_calls</li> <li>Overlay: annotations_displayed, obs_connected</li> </ul>"},{"location":"tutorials/admin-ui/#live-transcript-panel","title":"Live Transcript Panel","text":"<p>The center panel shows real-time transcription output:</p> <pre><code>[12:34:56] Hello everyone, welcome to the stream\n[12:34:59] Today we're going to talk about machine learning\n[12:35:03] Specifically, we'll cover neural networks\n</code></pre> <p>The transcript updates as audio is processed. This is useful for:</p> <ul> <li>Verifying transcription is working</li> <li>Checking transcription quality</li> <li>Debugging audio issues</li> </ul>"},{"location":"tutorials/admin-ui/#recent-annotations-panel","title":"Recent Annotations Panel","text":"<p>The right panel shows generated annotations:</p> <p>Each annotation card shows:</p> <ul> <li>Term - The extracted key term (bold)</li> <li>Explanation - Brief educational explanation</li> </ul> <p>Annotations appear here before being sent to OBS. The panel shows the 10 most recent annotations.</p>"},{"location":"tutorials/admin-ui/#settings-panel","title":"Settings Panel","text":"<p>Click the Settings expansion panel to configure the pipeline.</p>"},{"location":"tutorials/admin-ui/#audio-ingest-settings","title":"Audio Ingest Settings","text":"Setting Description Default SRT Port Port for SRT listener 9998 Latency (ms) SRT buffer latency 200"},{"location":"tutorials/admin-ui/#transcriber-settings","title":"Transcriber Settings","text":"Setting Description Default Model Whisper model to use mlx-community/whisper-base-mlx"},{"location":"tutorials/admin-ui/#annotator-settings","title":"Annotator Settings","text":"Setting Description Default Provider LLM provider openai Model LLM model gpt-4o-mini"},{"location":"tutorials/admin-ui/#obs-overlay-settings","title":"OBS Overlay Settings","text":"Setting Description Default Host OBS WebSocket host localhost Port OBS WebSocket port 4455 <p>Settings Changes</p> <p>Changes to settings take effect on the next pipeline start. You may need to stop and restart the pipeline for changes to apply.</p>"},{"location":"tutorials/admin-ui/#common-workflows","title":"Common Workflows","text":""},{"location":"tutorials/admin-ui/#starting-fresh","title":"Starting Fresh","text":"<ol> <li>Open the admin UI</li> <li>Verify all actors show grey (stopped)</li> <li>Configure settings if needed</li> <li>Click \"Start Pipeline\"</li> <li>Wait for all actors to turn green</li> </ol>"},{"location":"tutorials/admin-ui/#monitoring-a-stream","title":"Monitoring a Stream","text":"<ol> <li>Start the pipeline</li> <li>Watch the Live Transcript for real-time text</li> <li>Check Recent Annotations for extracted terms</li> <li>Monitor actor stats for throughput</li> </ol>"},{"location":"tutorials/admin-ui/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Check actor status for red indicators</li> <li>Hover over error indicators for details</li> <li>Check terminal output for full error messages</li> <li>Stop and restart individual problem actors (future feature)</li> </ol>"},{"location":"tutorials/admin-ui/#whats-next","title":"What's Next?","text":"<ul> <li>Configure OBS for SRT - Set up OBS properly</li> <li>Tune Transcription - Improve accuracy</li> <li>Configuration Reference - All settings</li> </ul>"},{"location":"tutorials/obs-setup/","title":"Setting Up OBS for popup-ai","text":"<p>This tutorial walks you through the complete OBS Studio setup for popup-ai, including:</p> <ul> <li>WebSocket connection for overlay control</li> <li>SRT streaming for audio capture</li> <li>Text sources for annotation display</li> </ul>"},{"location":"tutorials/obs-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>OBS Studio 28.0 or later - WebSocket 5.x is built-in</li> <li>popup-ai installed - See Quickstart</li> </ul> <p>OBS Version</p> <p>OBS Studio 28+ includes WebSocket support natively. You do not need to install a separate WebSocket plugin. If you have an older obs-websocket plugin installed, remove it to avoid conflicts.</p>"},{"location":"tutorials/obs-setup/#part-1-websocket-configuration","title":"Part 1: WebSocket Configuration","text":"<p>The WebSocket connection allows popup-ai to control OBS text sources for displaying annotations.</p>"},{"location":"tutorials/obs-setup/#step-1-open-websocket-settings","title":"Step 1: Open WebSocket Settings","text":"<ol> <li>Open OBS Studio</li> <li>Go to Tools \u2192 WebSocket Server Settings</li> </ol>"},{"location":"tutorials/obs-setup/#step-2-enable-the-server","title":"Step 2: Enable the Server","text":"<ol> <li>Check Enable WebSocket server</li> <li>Note the Server Port (default: <code>4455</code>)</li> <li>Check Enable Authentication (recommended)</li> <li>Copy or note the generated Server Password</li> <li>Click Apply</li> </ol> <p>Password</p> <p>OBS generates a secure password automatically. Copy it now - you'll need it for popup-ai configuration.</p>"},{"location":"tutorials/obs-setup/#step-3-verify-connection-status","title":"Step 3: Verify Connection Status","text":"<p>The dialog should show a green status indicator when the server is running.</p>"},{"location":"tutorials/obs-setup/#step-4-configure-popup-ai","title":"Step 4: Configure popup-ai","text":"<p>Set the WebSocket credentials via environment variables:</p> <pre><code>export POPUP_OVERLAY_OBS_HOST=localhost\nexport POPUP_OVERLAY_OBS_PORT=4455\nexport POPUP_OVERLAY_OBS_PASSWORD=\"your-password-here\"\n</code></pre> <p>Or in the admin UI Settings panel under \"OBS Overlay\".</p>"},{"location":"tutorials/obs-setup/#part-2-scene-and-source-setup","title":"Part 2: Scene and Source Setup","text":"<p>popup-ai displays annotations using text sources in OBS. You can set these up manually for more control, or let popup-ai create them automatically.</p>"},{"location":"tutorials/obs-setup/#option-a-automatic-setup-recommended","title":"Option A: Automatic Setup (Recommended)","text":"<p>popup-ai attempts to create text sources automatically when the Overlay actor starts. It will create:</p> <ul> <li><code>popup-ai-slot-1</code></li> <li><code>popup-ai-slot-2</code></li> <li><code>popup-ai-slot-3</code></li> <li><code>popup-ai-slot-4</code></li> </ul> <p>For this to work:</p> <ol> <li>Create a scene named <code>popup-ai-overlay</code> (or set a different name via <code>POPUP_OVERLAY_SCENE_NAME</code>)</li> <li>Make sure the scene is selected when starting popup-ai</li> </ol> <p>Approximation</p> <p>Automatic source creation depends on OBS version and platform. If it fails, use manual setup below.</p>"},{"location":"tutorials/obs-setup/#option-b-manual-setup","title":"Option B: Manual Setup","text":"<p>Create the text sources yourself for full control over positioning and styling.</p>"},{"location":"tutorials/obs-setup/#step-1-create-a-scene","title":"Step 1: Create a Scene","text":"<ol> <li>In Scenes, click + to add a new scene</li> <li>Name it <code>popup-ai-overlay</code></li> </ol>"},{"location":"tutorials/obs-setup/#step-2-add-text-sources","title":"Step 2: Add Text Sources","text":"<p>For each slot (1-4):</p> <ol> <li>In Sources, click + \u2192 Text (GDI+) (Windows) or Text (FreeType 2) (macOS/Linux)</li> <li>Name it exactly: <code>popup-ai-slot-1</code> (then 2, 3, 4)</li> <li>Click OK</li> </ol>"},{"location":"tutorials/obs-setup/#step-3-configure-text-properties","title":"Step 3: Configure Text Properties","text":"<p>For each text source:</p> <ol> <li>Font: Choose a readable font (e.g., Arial, 24pt)</li> <li>Color: White or contrasting color for your scene</li> <li>Outline: Optional, helps visibility</li> <li>Leave Text empty (popup-ai will fill it)</li> </ol>"},{"location":"tutorials/obs-setup/#step-4-position-the-sources","title":"Step 4: Position the Sources","text":"<p>Arrange the 4 text sources where you want annotations to appear:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                     \u2502\n\u2502  [slot-1]              [slot-2]     \u2502\n\u2502                                     \u2502\n\u2502         Your Stream Content         \u2502\n\u2502                                     \u2502\n\u2502  [slot-3]              [slot-4]     \u2502\n\u2502                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Positioning</p> <ul> <li>Corner positions work well for non-intrusive annotations</li> <li>Consider using lower-third style for educational content</li> <li>Test visibility against your typical stream content</li> </ul>"},{"location":"tutorials/obs-setup/#suggested-text-source-settings","title":"Suggested Text Source Settings","text":"Property Recommended Value Font Arial, Helvetica, or system sans-serif Size 24-32pt (adjust for your resolution) Color White (#FFFFFF) Outline 2px black (improves readability) Background Semi-transparent black (optional) Word Wrap Enabled, ~400px width"},{"location":"tutorials/obs-setup/#part-3-srt-audio-streaming","title":"Part 3: SRT Audio Streaming","text":"<p>SRT (Secure Reliable Transport) sends your audio to popup-ai for transcription.</p>"},{"location":"tutorials/obs-setup/#step-1-configure-stream-output","title":"Step 1: Configure Stream Output","text":"<ol> <li>Go to Settings \u2192 Stream</li> <li>Set Service to <code>Custom...</code></li> <li>Set Server to:</li> </ol> <pre><code>srt://localhost:9998?mode=caller&amp;latency=200000\n</code></pre> <ol> <li>Leave Stream Key empty</li> <li>Click Apply</li> </ol>"},{"location":"tutorials/obs-setup/#step-2-configure-audio-encoding","title":"Step 2: Configure Audio Encoding","text":"<ol> <li>Go to Settings \u2192 Output \u2192 Streaming</li> <li>Set Audio Encoder to <code>AAC</code></li> <li>Set Audio Bitrate to <code>128</code> Kbps (or higher)</li> </ol>"},{"location":"tutorials/obs-setup/#step-3-verify-audio-sources","title":"Step 3: Verify Audio Sources","text":"<p>In the main OBS window:</p> <ol> <li>Check your Audio Mixer panel</li> <li>Ensure microphone/audio input is active</li> <li>Speak and verify levels move</li> </ol>"},{"location":"tutorials/obs-setup/#srt-url-parameters","title":"SRT URL Parameters","text":"Parameter Value Description <code>srt://</code> Protocol SRT streaming <code>localhost</code> Host popup-ai location <code>9998</code> Port SRT listener port <code>mode=caller</code> Mode OBS initiates connection <code>latency=200000</code> Buffer 200ms (microseconds) <p>Port Conflicts</p> <p>If port 9998 is in use, change it in both: - OBS SRT URL - popup-ai config: <code>POPUP_AUDIO_SRT_PORT</code></p>"},{"location":"tutorials/obs-setup/#part-4-testing-the-setup","title":"Part 4: Testing the Setup","text":""},{"location":"tutorials/obs-setup/#step-1-start-popup-ai","title":"Step 1: Start popup-ai","text":"<pre><code>uv run popup-ai\n</code></pre> <p>Wait for the admin UI to open at <code>http://127.0.0.1:8080</code>.</p>"},{"location":"tutorials/obs-setup/#step-2-check-initial-state","title":"Step 2: Check Initial State","text":"<ul> <li>All actors should show grey (stopped)</li> <li>No errors in terminal</li> </ul>"},{"location":"tutorials/obs-setup/#step-3-start-streaming-in-obs","title":"Step 3: Start Streaming in OBS","text":"<ol> <li>Select your <code>popup-ai-overlay</code> scene</li> <li>Click Start Streaming</li> <li>Watch OBS status bar for connection</li> </ol>"},{"location":"tutorials/obs-setup/#step-4-start-the-pipeline","title":"Step 4: Start the Pipeline","text":"<ol> <li>In popup-ai admin UI, click Start Pipeline</li> <li>Watch actors turn green one by one</li> </ol>"},{"location":"tutorials/obs-setup/#step-5-verify-each-component","title":"Step 5: Verify Each Component","text":"Component How to Verify Audio Ingest <code>chunks_sent</code> increasing Transcriber Text appearing in Live Transcript Annotator Annotations in Recent Annotations panel Overlay Text appearing in OBS sources"},{"location":"tutorials/obs-setup/#complete-setup-checklist","title":"Complete Setup Checklist","text":"<ul> <li>[ ] OBS Studio 28+ installed</li> <li>[ ] WebSocket server enabled (Tools \u2192 WebSocket Server Settings)</li> <li>[ ] WebSocket password configured in popup-ai</li> <li>[ ] Scene <code>popup-ai-overlay</code> created</li> <li>[ ] Text sources <code>popup-ai-slot-1</code> through <code>popup-ai-slot-4</code> created</li> <li>[ ] Text sources positioned and styled</li> <li>[ ] SRT stream configured (Custom service, srt://localhost:9998)</li> <li>[ ] Audio encoder set to AAC</li> <li>[ ] Audio sources active in mixer</li> <li>[ ] popup-ai can connect to OBS (Overlay actor green)</li> <li>[ ] Transcription working (text in Live Transcript)</li> <li>[ ] Annotations appearing in OBS</li> </ul>"},{"location":"tutorials/obs-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/obs-setup/#websocket-connection-issues","title":"WebSocket Connection Issues","text":"<p>\"OBS not connected\" in popup-ai</p> <ol> <li>Verify WebSocket server is enabled in OBS</li> <li>Check port matches (default 4455)</li> <li>Verify password is correct</li> <li>Check no firewall blocking the port</li> </ol> <p>Overlay actor shows error</p> <ol> <li>Check OBS is running</li> <li>Verify scene name matches configuration</li> <li>Try restarting both OBS and popup-ai</li> </ol>"},{"location":"tutorials/obs-setup/#srt-streaming-issues","title":"SRT Streaming Issues","text":"<p>\"Connection refused\" when starting stream</p> <ol> <li>Ensure popup-ai is running before OBS starts streaming</li> <li>Verify SRT port (9998) is not in use</li> <li>Check <code>mode=caller</code> in SRT URL</li> </ol> <p>Audio glitches</p> <ol> <li>Increase latency: <code>latency=300000</code> or higher</li> <li>Check network stability</li> <li>Reduce OBS encoding load</li> </ol>"},{"location":"tutorials/obs-setup/#text-source-issues","title":"Text Source Issues","text":"<p>Sources not created automatically</p> <ol> <li>Create them manually (see Option B above)</li> <li>Ensure scene name matches exactly</li> </ol> <p>Text not appearing</p> <ol> <li>Check source is visible (eye icon in Sources)</li> <li>Verify source is in the active scene</li> <li>Check text color contrasts with background</li> </ol>"},{"location":"tutorials/obs-setup/#network-considerations","title":"Network Considerations","text":""},{"location":"tutorials/obs-setup/#local-setup-same-machine","title":"Local Setup (Same Machine)","text":"<p>Use <code>localhost</code> for both WebSocket and SRT:</p> <ul> <li>WebSocket: <code>localhost:4455</code></li> <li>SRT: <code>srt://localhost:9998</code></li> </ul>"},{"location":"tutorials/obs-setup/#remote-setup-different-machines","title":"Remote Setup (Different Machines)","text":"<p>Replace <code>localhost</code> with the popup-ai machine's IP:</p> <ul> <li>WebSocket: <code>192.168.1.100:4455</code></li> <li>SRT: <code>srt://192.168.1.100:9998</code></li> </ul> <p>Ensure ports 4455 (WebSocket) and 9998 (SRT) are open on the popup-ai machine.</p>"},{"location":"tutorials/obs-setup/#whats-next","title":"What's Next?","text":"<ul> <li>Tune Transcription - Improve accuracy</li> <li>Understanding the Admin UI - Learn the controls</li> <li>Configuration Reference - All settings</li> </ul>"},{"location":"tutorials/obs-setup/#references","title":"References","text":"<ul> <li>OBS Remote Control Guide</li> <li>OBS WebSocket GitHub</li> <li>SRT Protocol</li> </ul>"},{"location":"tutorials/quickstart/","title":"Quickstart","text":"<p>Get popup-ai running and see your first annotation in about 5 minutes.</p>"},{"location":"tutorials/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ul> <li>[x] macOS with Apple Silicon (M1/M2/M3)</li> <li>[x] Python 3.13+ (<code>python3 --version</code>)</li> <li>[x] OBS Studio installed</li> <li>[x] ffmpeg with SRT support (see below)</li> <li>[x] An OpenAI API key (for annotations)</li> </ul>"},{"location":"tutorials/quickstart/#installing-ffmpeg-with-srt-support","title":"Installing ffmpeg with SRT Support","text":"<p>The standard Homebrew ffmpeg does not include SRT protocol support. You need to install from the <code>homebrew-ffmpeg</code> tap:</p> <pre><code># Add the tap\nbrew tap homebrew-ffmpeg/ffmpeg\n\n# Install ffmpeg with SRT support\nbrew install homebrew-ffmpeg/ffmpeg/ffmpeg --with-srt\n</code></pre> <p>If you already have ffmpeg installed, unlink it first:</p> <pre><code>brew unlink ffmpeg\nbrew install homebrew-ffmpeg/ffmpeg/ffmpeg --with-srt\n</code></pre> <p>Verify SRT support is enabled:</p> <pre><code>ffmpeg -protocols 2&gt;&amp;1 | grep srt\n</code></pre> <p>You should see <code>srt</code> in the output.</p> <p>SRT Required</p> <p>Without SRT support, the audio pipeline cannot receive streams from OBS. See Installing ffmpeg for detailed instructions.</p>"},{"location":"tutorials/quickstart/#step-1-install-popup-ai","title":"Step 1: Install popup-ai","text":"<p>Install popup-ai with all optional dependencies:</p> <pre><code># Using uv (recommended)\nuv pip install 'popup-ai[all]'\n\n# Or using pip\npip install 'popup-ai[all]'\n</code></pre> <p>This installs:</p> <ul> <li>Core dependencies (Ray, Pydantic, Typer)</li> <li>UI extras (NiceGUI)</li> <li>LLM extras (pydantic-ai)</li> <li>OBS extras (obsws-python)</li> </ul>"},{"location":"tutorials/quickstart/#step-2-set-your-api-key","title":"Step 2: Set Your API Key","text":"<p>popup-ai uses pydantic-ai for LLM annotations. Set your OpenAI API key:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre> <p>Other Providers</p> <p>You can use other LLM providers by setting <code>POPUP_ANNOTATOR_PROVIDER</code> and <code>POPUP_ANNOTATOR_MODEL</code>. See Configuration.</p>"},{"location":"tutorials/quickstart/#step-3-start-popup-ai","title":"Step 3: Start popup-ai","text":"<p>Launch the application:</p> <pre><code>uv run popup-ai\n</code></pre> <p>You should see:</p> <pre><code>Initializing Ray...\nStarting popup-ai with UI...\n</code></pre> <p>Your browser will open to <code>http://127.0.0.1:8080</code> showing the admin UI.</p> <p></p> <p>UI Not Opening?</p> <p>If the browser doesn't open automatically, navigate to <code>http://127.0.0.1:8080</code> manually.</p>"},{"location":"tutorials/quickstart/#step-4-configure-obs-for-srt","title":"Step 4: Configure OBS for SRT","text":"<p>Open OBS Studio and configure it to stream audio via SRT:</p> <ol> <li>Go to Settings \u2192 Stream</li> <li>Set Service to <code>Custom...</code></li> <li>Set Server to:    <pre><code>srt://localhost:9998?mode=caller&amp;latency=200000\n</code></pre></li> <li>Click Apply</li> </ol> <p>What's Happening?</p> <p>OBS will connect to popup-ai's SRT listener on port 9998. The <code>latency=200000</code> sets a 200ms buffer for smooth streaming.</p>"},{"location":"tutorials/quickstart/#step-5-start-streaming","title":"Step 5: Start Streaming","text":"<ol> <li>In OBS, click Start Streaming</li> <li>In the popup-ai admin UI, click Start Pipeline</li> </ol> <p>You should see:</p> <ul> <li>Actor status indicators turn green</li> <li>Transcripts appearing in the \"Live Transcript\" panel</li> <li>Annotations appearing in the \"Recent Annotations\" panel</li> </ul> <p></p>"},{"location":"tutorials/quickstart/#step-6-check-obs-overlay","title":"Step 6: Check OBS Overlay","text":"<p>popup-ai creates text sources in OBS for displaying annotations:</p> <ul> <li><code>popup-ai-slot-1</code> through <code>popup-ai-slot-4</code></li> </ul> <p>Add these sources to your scene to see annotations on stream.</p>"},{"location":"tutorials/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Understanding the Admin UI - Learn all the UI features</li> <li>Configure OBS for SRT - Detailed OBS setup</li> <li>Tune Transcription - Improve accuracy</li> <li>Architecture - How it all works</li> </ul>"},{"location":"tutorials/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/quickstart/#connection-refused-when-obs-tries-to-stream","title":"\"Connection refused\" when OBS tries to stream","text":"<p>Make sure popup-ai is running before starting the OBS stream. The SRT listener needs to be active.</p>"},{"location":"tutorials/quickstart/#no-transcripts-appearing","title":"No transcripts appearing","text":"<ol> <li>Check that OBS is actually streaming (look for bitrate in OBS status bar)</li> <li>Verify audio is being captured (check OBS audio meters)</li> <li>Check actor status in admin UI - look for errors</li> </ol>"},{"location":"tutorials/quickstart/#annotations-not-generating","title":"Annotations not generating","text":"<ol> <li>Verify your <code>OPENAI_API_KEY</code> is set correctly</li> <li>Check the annotator actor status for errors</li> <li>Look at the terminal output for API errors</li> </ol>"}]}